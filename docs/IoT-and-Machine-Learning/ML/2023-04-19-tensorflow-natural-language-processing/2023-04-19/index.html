<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/index" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.0.0">
<title data-rh="true">(Re) Introduction to Tensorflow Natural Language Processing | Mike Polinowski</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/2023-04-19"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="(Re) Introduction to Tensorflow Natural Language Processing | Mike Polinowski"><meta data-rh="true" name="description" content="Using Tensorflow to classify Disaster Tweet."><meta data-rh="true" property="og:description" content="Using Tensorflow to classify Disaster Tweet."><link data-rh="true" rel="icon" href="/img/icons/favicon-32x32.png"><link data-rh="true" rel="canonical" href="https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/2023-04-19"><link data-rh="true" rel="alternate" href="https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/2023-04-19" hreflang="en"><link data-rh="true" rel="alternate" href="https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/2023-04-19" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Mike Polinowski RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Mike Polinowski Atom Feed">




<link rel="icon" href="/img/angular_momentum.png">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="rgb(37,194,160)">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#000">
<link rel="apple-touch-icon" href="/img/angular_momentum.png">
<link rel="mask-icon" href="/img/angular_momentum.png" color="rgb(33,33,33)">
<meta name="msapplication-TileImage" content="/img/angular_momentum.png">
<meta name="msapplication-TileColor" content="#000">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P74BDWF0C6"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-P74BDWF0C6",{})</script><link rel="stylesheet" href="/assets/css/styles.dcca9435.css">
<script src="/assets/js/runtime~main.9a0cc3b5.js" defer="defer"></script>
<script src="/assets/js/main.6d14a3ae.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_gu5v" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/angular_momentum.png" alt="Mike Polinowski :: Dev Notebook" class="themedComponent_ZRzL themedComponent--light_dGsa"><img src="/img/angular_momentum.png" alt="Mike Polinowski :: Dev Notebook" class="themedComponent_ZRzL themedComponent--dark_pzCA"></div><b class="navbar__title text--truncate">Mike Polinowski</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/docs/tags">Tags</a><a class="navbar__item navbar__link" href="/Search">Search</a><a href="https://mpolinowski.github.io/Personal" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">About<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/mpolinowski" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_kWbt colorModeToggle_GwZs"><button class="clean-btn toggleButton_fOL9 toggleButtonDisabled_STpu" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_DCeJ"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_DFgp"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_IP3a"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_IbdI"><div class="docsWrapper_JGIH"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_SdI4" type="button"></button><div class="docRoot_eRbX"><aside class="theme-doc-sidebar-container docSidebarContainer_Ta75"><div class="sidebarViewport_fgog"><div class="sidebar_oDHW"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_vPEQ"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/development">Development</a><button aria-label="Expand sidebar category &#x27;Development&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/devops">DevOps</a><button aria-label="Expand sidebar category &#x27;DevOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/docs/category/machine-learning-ai-and-computer-vision">Machine Learning, AI and Computer Vision</a><button aria-label="Collapse sidebar category &#x27;Machine Learning, AI and Computer Vision&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/category/machine-learning">Machine Learning</a><button aria-label="Collapse sidebar category &#x27;Machine Learning&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-10-01--delib-face-detection/2023-10-01">DLIB Face Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-23--yolo8-listen/2023-09-23">Audio Classification with Computer Vision</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-21--cvat-automatic-annotation/2023-09-21">CVAT Semi-automatic and Automatic Annotation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-19--cvat-computer-vision-annotation-tool/2023-09-19">Computer Vision Annotation Tool (CVAT) Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-17--yolo8-nightshift/2023-09-17">YOLOv8 Nightshift</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-15--yolo8-tracking-and-ocr/2023-09-15">YOLOv8 License Plate Detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-10--model-explainability-shap/2023-09-11">Scikit-Learn ML Model Explainability</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-05--semantic-segmentation-in-opencv/2023-09-05">Using Tensorflow Models in OpenCV</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-09-01--yolo-i-know-flowers/2023-09-01">YOLOv8 Image Classifier</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-31--semantic_segmentation_detectron2_openimages_dataset/2023-08-31">Detectron Object Detection with OpenImages Dataset (WIP)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-30--instance_segmentation_detectron2_model_zoo_mask_rcnn/2023-08-30">Instance Segmentation with PyTorch (Mask RCNN)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-29--semantic-segmentation-detectron2-model-zoo-faster-rcnn/2023-08-29">Image Segmentation with PyTorch (Faster RCNN)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-28--semantic-segmentation-detectron2-model-zoo/2023-08-28">Image Segmentation with PyTorch (RCNN)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-27--image-segmentation-with-pytorch/2023-08-27">Image Segmentation with PyTorch</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-21--pytorch-development-in-docker/2023-08-21">Containerized PyTorch Dev Workflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-13-tensorflow-i-know-flowers-model-eval/2023-08-13">Tensorflow Image Classifier - Model Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-12-tensorflow-i-know-flowers-xception/2023-08-12">Tensorflow Image Classifier - Xception</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-11-tensorflow-i-know-flowers-vit/2023-08-11">Tensorflow Image Classifier - ViT</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-10-tensorflow-i-know-flowers-nasnetmobile/2023-08-10">Tensorflow Image Classifier - NASNetMobile</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-09-tensorflow-i-know-flowers-mobilenetv3small/2023-08-09">Tensorflow Image Classifier - MobileNetV3Small</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-08-tensorflow-i-know-flowers-mobilenetv3large/2023-08-08">Tensorflow Image Classifier - MobileNetV3Large</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-07-tensorflow-i-know-flowers-mobilenetv2/2023-08-07">Tensorflow Image Classifier - MobileNetV2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-06-tensorflow-i-know-flowers-inceptionv3/2023-08-06">Tensorflow Image Classifier - InceptionV3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-05-tensorflow-i-know-flowers-efficientnetv2s/2023-08-05">Tensorflow Image Classifier - EfficientNetV2S</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-04-tensorflow-i-know-flowers-efficientnetv2b0/2023-08-04">Tensorflow Image Classifier - EfficientNetV2B0</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-03-tensorflow-i-know-flowers-deit/2023-08-03">Tensorflow Image Classifier - Data-efficient Image Transformers</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-02-tensorflow-i-know-flowers-preprocessing/2023-08-02">Tensorflow Image Classifier - Data Pre-processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-08-01-tensorflow-i-know-flowers-intro/2023-08-01">Tensorflow Image Classifier - Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-27-tensorflow-vision-transformer/2023-07-27">Tensorflow VITs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-26-tensorflow-human-emotion-detector/2023-07-26">Human Emotion Detection with Tensorflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-25-onnx-models/2023-07-25">Working with ONNX Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-21-introduction-to-pytorch-caffe2/2023-07-21">Introduction to Caffe2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-07-02-sql-in-data-science-ml/2023-07-02">SQL in Data Science - Machine Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-06-30-sql-in-data-science-advanced/2023-06-30">SQL in Data Science - Slightly more Advanced Queries</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-06-27-sql-in-data-science-basics/2023-06-27">SQL in Data Science - The Basics using Python</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-06-26-autogluon-transit-photometry-dataset/2023-06-26">Detection of Exoplanets using Transit Photometry</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/2023-04-19">(Re) Introduction to Tensorflow Natural Language Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-16-deep-3d-image-segmentation/2023-04-16">3D Image Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-14-manifold-learning-for-image-segmentation/2023-04-14">Dimensionality Reduction for Image Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-13-fisher-discriminant-analysis/2023-04-13">Fisher Linear Discriminant Analysis (LDA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-13-isometric-mapping/2023-04-13">Isometric Mapping (ISOMAP)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-13-multi-dimensional-scaling/2023-04-13">Multidimensional Scaling (MDS)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-12-tstochastic-neighbor-embedding/2023-04-12">tStochastic Neighbor Embedding (t-SNE)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-11-locally-linear-embedding/2023-04-11">Locally Linear Embedding (LLE)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-04-09-principal-component-analysis/2023-04-09">Principal Component Analysis (PCA)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-generative-adversial-networks/2023-03-26">Tensorflow 2 - Unsupervised Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-26-tensorflow-unsupervised-learning-autoencoders-super-resolution/2023-03-26">Tensorflow 2 - Unsupervised Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-24-tensorflow-unsupervised-learning-autoencoders/2023-03-24">Tensorflow 2 - Unsupervised Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-16-tensorflow-transfer-learning-scaling/2023-03-16">Tensorflow 2 - Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-11-tensorflow-transfer-learning-fine-tuning/2023-03-11">Tensorflow 2 - Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-06-tensorflow-transfer-learning-feature-extraction/2023-03-06">Tensorflow 2 - Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-05-tensorflow-convolutional-neural-network-multiclass-classifications/2023-03-05">Tensorflow 2 - Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-03-tensorflow-convolutional-neural-network-binary-classifications/2023-03-03">Tensorflow 2 - Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-03-02-tensorflow-neural-network-multi-classification/2023-03-02">Tensorflow 2 - Neural Network Classifications</a><button aria-label="Expand sidebar category &#x27;Tensorflow 2 - Neural Network Classifications&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-28-tensorflow-neural-network-classification-model-evaluation/2023-02-28">Tensorflow 2 - Neural Network Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-27-tensorflow-neural-network-classification/2023-02-27">Tensorflow 2 - Neural Network Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-26-tensorflow-neural-network-regression-data-preprocessing/2023-02-26">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-26-tensorflow-neural-network-regression-real-dataset/2023-02-26">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-25-tensorflow-neural-network-regression-experiments/2023-02-25">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-24-tensorflow-neural-network-regression-evaluation/2023-02-24">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-23-tensorflow-neural-network-regression/2023-02-23">Tensorflow 2 - Neural Network Regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-22-tensorflow-tensors-3/2023-02-22">Tensorflow 2 - An (Re)Introduction 2023 (3)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-21-tensorflow-tensors-2/2023-02-21">Tensorflow 2 - An (Re)Introduction 2023 (2)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-19-tensorflow-introduction/2023-02-19">Tensorflow 2 - An (Re)Introduction 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-vgg16/2023-02-18">Keras for Tensorflow - VGG16 Network Architecture</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-18-keras-introduction-rnn/2023-02-18">Keras for Tensorflow - Recurrent Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-17-keras-introduction-cnn/2023-02-17">Keras for Tensorflow - Convolutional Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-16-keras-introduction-ann/2023-02-16">Keras for Tensorflow - Artificial Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-15-as-one-yolo-object-tracking/2023-02-15">YOLOv8 with AS-One</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-02-14-keras-introduction/2023-02-14">Keras for Tensorflow - An (Re)Introduction 2023</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-30-predicting-wine-quality/2023-01-30">SciKit Wine Quality</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-28-opencv-coin-counter/2023-01-28">OpenCV Count My Money</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-14-yolov7_to_tensorflow/2023-01-14">YOLOv7 to Tensorflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-13-yolov7_data_conversion/2023-01-13">YOLOv7 Label Conversion</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-10-yolov7_custom_data/2023-01-10">YOLOv7 Training with Custom Data</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-08-depth-vision-midas/2023-01-08">MiDaS Depth Vision</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2023-01-05-yolov7/2023-01-05">YOLOv7 Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-31-tf-rnn-text-generation/2022-12-31">Recurrent Neural Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-28-tf-gan-image-generator/2022-12-28">Deep Convolutional Generative Adversarial Network</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-downsampling/2022-12-21">Tensorflow Downsampling</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-21-tf-deepdream/2022-12-21">Tensorflow Deep Dream</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-20-tf-representation/2022-12-19">Tensorflow Representation Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-19-tf-hub/2022-12-19">Tensorflow Hub</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-18-tf-transfer-learning/2022-12-18">Tensorflow Transfer Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-16-tf-cifar/2022-12-16">Tensorflow Image Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part6/2022-12-12">Breast Histopathology Image Segmentation Part 6</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-12-tf-breast-cancer-classification-part5/2022-12-12">Breast Histopathology Image Segmentation Part 5</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part4/2022-12-11">Breast Histopathology Image Segmentation Part 4</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part3/2022-12-11">Breast Histopathology Image Segmentation Part 3</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-11-tf-breast-cancer-classification-part2/2022-12-11">Breast Histopathology Image Segmentation Part 2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-12-10-tf-breast-cancer-classification-part1/2022-12-10">Breast Histopathology Image Segmentation Part 1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-11-27-containerized-deep-learning/2022-11-27">Deep Docker on Arch</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-04-04-pytorch-face-restoration/2022-04-04">Face Restoration with GFPGAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-04-03-pytorch-real-super-resolution/2022-04-03">Super Resolution with Real-ESRGAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-04-02-pytorch-super-resolution/2022-04-02">Super Resolution with ESRGAN</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-04-01-tensorflow-audio-classifier/2022-04-01">Deep Audio</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-20--yolo-app-yolov5-data-prep/2022-02-20">Yolo App - YOLOv5 Data Preparation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-19--yolo-app-flask/2022-02-19">Yolo App - Flask Web Application</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-18--yolo-app-ocr/2022-02-18">Yolo App - Tesseract Optical Character Recognition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-17--yolo-app-prediction-pipeline/2022-02-17">Yolo App - Pipeline Predictions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-16--yolo-app-tensorflow-model/2022-02-16">Yolo App - Train a Model with Tensorflow</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2022-02-15--yolo-app-get-data/2022-02-15">Yolo App - Data Collection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-10--opencv-optical-flow-tracking/2021-12-10">OpenCV Optical Flow Algorithm for Object Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-09--opencv-camshift-tracking/2021-12-09">OpenCV CAMshift Algorithm for Object Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-08--opencv-meanshift-tracking/2021-12-08">OpenCV Meanshift Algorithm for Object Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-07--opencv-detection-and-tracking/2021-12-07">OpenCV Object Detection and Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-06--opencv-object-tracking/2021-12-06">OpenCV Object Tracking</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-05--opencv-face-detection/2021-12-05">OpenCV Face Detection and Privacy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-04--opencv-image-objects/2021-12-04">OpenCV Image Objects</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-03--opencv-image-operations/2021-12-03">OpenCV Image Operations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-02--opencv-with-videos/2021-12-02">OpenCV, Streams and Video Files</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-12-01--opencv-with-images/2021-12-01">OpenCV and Images</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-15--facebook-prophet-introduction/2021-11-15">Introduction into FB Prophet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-14--tensorflow-model-for-tfjs/2021-11-14">Tensorflow.js React App</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-13--tensorflow-model-zoo/2021-11-13">Tensorflow2 Model Zoo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-12--tensorflow-crash-course-part-v/2021-11-12">Tensorflow2 Crash Course - Part V</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-11--tensorflow-crash-course-part-iv/2021-11-11">Tensorflow2 Crash Course - Part IV</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-10--tensorflow-crash-course-part-iii/2021-11-10">Tensorflow2 Crash Course - Part III</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-09--tensorflow-crash-course-part-ii/2021-11-09">Tensorflow2 Crash Course - Part II</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-08--tensorflow-crash-course-part-i/2021-11-08">Tensorflow Crash Course - Part I</a><button aria-label="Expand sidebar category &#x27;Tensorflow Crash Course - Part I&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-07--opencv-crash-course-part-ii/2021-11-07">OpenCV Crash Course Part II</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-06--opencv-crash-course-part-i/2021-11-06">OpenCV Crash Course Part I</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-05--license-plates-yolov4-opencv-tesseract/2021-11-05">License Plate Recognition with YOLOv4, OpenCV and Tesseract</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-04--installing-yolov4/2021-11-04">Installing YOLOv4 with Anaconda</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-03--streamlit-opencv-mediapipe/2021-11-03">Streamlit user interface for openCV/Mediapipe face mesh app</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-02--spacy_ner_predictions/2021-11-02">spaCy NER Predictions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-11-01--spacy_natural_language_processing/2021-11-01">spaCy NER on Arch Linux</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2019-04-01--introduction-to-keras/2019-04-01">Introduction to Keras</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2021-10-31--tesseract_ocr_arch_linux/2021-10-31">Tesseract OCR on Arch Linux</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2019-03-31--introduction-to-tensorflow-2-beta/2019-03-31">Introduction to TensorFlow 2 Beta</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/IoT-and-Machine-Learning/ML/2018-01-02--machine-learning-with-python/2018-01-02">Machine Learning with SciKit Learn</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/aiops">AIOps</a><button aria-label="Expand sidebar category &#x27;AIOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/automation-deep-vision-and-robotics">Automation, Deep Vision and Robotics</a><button aria-label="Expand sidebar category &#x27;Automation, Deep Vision and Robotics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_lg0V"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_nDJs"><div class="docItemContainer_OGiL"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_k3Z9" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_JACu"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/machine-learning-ai-and-computer-vision"><span itemprop="name">Machine Learning, AI and Computer Vision</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/machine-learning"><span itemprop="name">Machine Learning</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">(Re) Introduction to Tensorflow Natural Language Processing</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_QCOD theme-doc-toc-mobile tocMobile_N0YI"><button type="button" class="clean-btn tocCollapsibleButton_pHwF">On this page</button></div><div class="theme-doc-markdown markdown"><p><img alt="Victoria Harbour, Hongkong" src="/assets/images/photo-kt443t6d_64hdh43hfh6dgjdfhg4_d-a3f1c3cf7621dc9c70d8bc62dec4a9d5.jpg" width="2385" height="823"></p>
<ul>
<li><a href="#re-introduction-to-tensorflow-natural-language-processing">(Re) Introduction to Tensorflow Natural Language Processing</a>
<ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#dataset">Dataset</a>
<ul>
<li><a href="#exploration">Exploration</a></li>
<li><a href="#train-test-split">Train Test Split</a></li>
<li><a href="#tokenization-and-embedding">Tokenization and Embedding</a>
<ul>
<li><a href="#tokenization">Tokenization</a></li>
<li><a href="#embedding">Embedding</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiments">Experiments</a>
<ul>
<li><a href="#model-0-naive-bayes-tf-ids">Model 0: Naive Bayes tf-ids</a>
<ul>
<li><a href="#tokenization-and-modelling-pipeline">Tokenization and Modelling Pipeline</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#predictions">Predictions</a></li>
<li><a href="#model-0-metrics">Model 0 Metrics</a></li>
</ul>
</li>
<li><a href="#model-1-simple-dense">Model 1: Simple Dense</a>
<ul>
<li><a href="#model-building-and-training">Model Building and Training</a></li>
<li><a href="#model-evaluation">Model Evaluation</a></li>
<li><a href="#model-1-metrics">Model 1 Metrics</a></li>
<li><a href="#visualize-the-embedding">Visualize the Embedding</a></li>
</ul>
</li>
<li><a href="#model-2-lstm-long-term-short-term-memory-rnn">Model 2: LSTM Long-term Short-term Memory RNN</a>
<ul>
<li><a href="#model-building-and-training-1">Model Building and Training</a></li>
<li><a href="#model-evaluation-1">Model Evaluation</a></li>
<li><a href="#model-2-metrics">Model 2 Metrics</a></li>
</ul>
</li>
<li><a href="#model-3-gru-gated-recurrent-unit-rnn">Model 3: GRU Gated Recurrent Unit RNN</a>
<ul>
<li><a href="#model-building-and-training-2">Model Building and Training</a></li>
<li><a href="#model-evaluation-2">Model Evaluation</a></li>
<li><a href="#model-3-metrics">Model 3 Metrics</a></li>
</ul>
</li>
<li><a href="#model-4-bi-directional-rnn">Model 4: Bi-Directional RNN</a>
<ul>
<li><a href="#model-building-and-training-3">Model Building and Training</a></li>
<li><a href="#model-evaluation-3">Model Evaluation</a></li>
<li><a href="#model-4-metrics">Model 4 Metrics</a></li>
</ul>
</li>
<li><a href="#model-5-conv1d">Model 5: Conv1D</a>
<ul>
<li><a href="#model-building-and-training-4">Model Building and Training</a></li>
<li><a href="#model-evaluation-4">Model Evaluation</a></li>
<li><a href="#model-5-metrics">Model 5 Metrics</a></li>
</ul>
</li>
<li><a href="#model-6-transfer-learning-feature-extractor">Model 6: Transfer Learning Feature Extractor</a>
<ul>
<li><a href="#model-building-and-training-5">Model Building and Training</a></li>
<li><a href="#model-evaluation-5">Model Evaluation</a></li>
<li><a href="#model-6-metrics">Model 6 Metrics</a></li>
</ul>
</li>
<li><a href="#model-6a-added-dense-layer">Model 6a (added Dense Layer)</a>
<ul>
<li><a href="#model-evaluation-6">Model Evaluation</a></li>
<li><a href="#model-6a-metrics">Model 6a Metrics</a></li>
</ul>
</li>
<li><a href="#model-6b-transfer-learning-feature-extractor-10-dataset">Model 6b: Transfer Learning Feature Extractor (10% Dataset)</a>
<ul>
<li><a href="#dataset-1">Dataset</a></li>
<li><a href="#model-building-and-training-6">Model Building and Training</a></li>
<li><a href="#model-evaluation-7">Model Evaluation</a></li>
<li><a href="#model-6b-metrics">Model 6b Metrics</a></li>
</ul>
</li>
<li><a href="#model-6c-use-data-leakage-issue-10-dataset">Model 6c: USE Data Leakage Issue (10% Dataset)</a>
<ul>
<li><a href="#data-leakage-issue">Data Leakage Issue</a>
<ul>
<li><a href="#training-10-split">Training 10% Split</a></li>
</ul>
</li>
<li><a href="#model-evaluation-8">Model Evaluation</a></li>
<li><a href="#model-6c-metrics">Model 6c Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#compare-experiments">Compare Experiments</a></li>
<li><a href="#saving--loading-trained-model">Saving &amp; Loading Trained Model</a>
<ul>
<li><a href="#hdf5-format-higher-compatibility-to-3rd-parties">HDF5 Format (Higher Compatibility to 3rd Parties)</a></li>
<li><a href="#saved-model-format-tensorflow-default">Saved Model Format (Tensorflow Default)</a></li>
</ul>
</li>
<li><a href="#best-model-evaluation">Best Model Evaluation</a>
<ul>
<li><a href="#false-positives">False Positives</a></li>
<li><a href="#false-negatives">False Negatives</a></li>
</ul>
</li>
<li><a href="#test-dataset-predictions">Test Dataset Predictions</a></li>
<li><a href="#speedscore-tradeoff">Speed/Score Tradeoff</a>
<ul>
<li><a href="#comparing-the-performance-of-all-models">Comparing the Performance of all Models</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a href="https://github.com/mpolinowski/tf-nlp-2023">Github Repository</a></p>
<h1 id="re-introduction-to-tensorflow-natural-language-processing">(Re) Introduction to Tensorflow Natural Language Processing</h1>
<h2 id="abstract">Abstract</h2>
<p>Twitter has become an important communication channel in times of emergency.
The ubiquitousness of smartphones enables people to announce an emergency theyre observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).</p>
<p>But, its not always clear whether a persons words are actually announcing a disaster. Here Machine Learning can help us segmenting large quantities of incoming messages. But what type Machine Learning is the best suited?</p>
<p>I am running a couple of experiments to show the performace of different solutions and their trade-off - how long does it take to get a prediction from a given algorithm? Is a higher accuracy worth the wait?</p>
<p><img alt="(Re) Introduction to Tensorflow Natural Language Processing" src="/assets/images/tf_nlp_desaster_tweets_06-458287cf210faec316cfb51972d6c1a9.png" width="1010" height="701"></p>
<h2 id="dataset">Dataset</h2>
<p><a href="https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip">https://www.kaggle.com/competitions/nlp-getting-started/data</a></p>
<pre><code>.
 data
  test.csv
  train.csv
</code></pre>
<pre><code class="language-python">import datetime
import io
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
import time
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
import tensorflow as tf
# tf-hub bug https://stackoverflow.com/questions/69339917/importerror-cannot-import-name-dnn-logit-fn-builder-from-partially-initialize
from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
import tensorflow_hub as hub
from tensorflow.keras.layers import (
    TextVectorization,
    Embedding,
    Input,
    Dense,
    GlobalAveragePooling1D,
    GlobalMaxPool1D,
    LSTM,
    GRU,
    Bidirectional,
    Conv1D
)
</code></pre>
<pre><code class="language-python">from helper_functions import (
    create_tensorboard_callback,
    plot_loss_curves,
    compare_histories,
    calculate_metrics,
    time_to_prediction
)
</code></pre>
<pre><code class="language-python">SEED = 42
LOG_DIR = &#x27;tensorboad&#x27;
</code></pre>
<h3 id="exploration">Exploration</h3>
<pre><code class="language-python">train_df = pd.read_csv(&#x27;data/train.csv&#x27;)
test_df = pd.read_csv(&#x27;data/test.csv&#x27;)

train_df.head(5)
# target 1 = disaster / 0 = not a disaster
</code></pre>
<table><thead><tr><th></th><th>id</th><th>keyword</th><th>location</th><th>text</th><th>target</th></tr></thead><tbody><tr><td>0</td><td>1</td><td>NaN</td><td>NaN</td><td>Our Deeds are the Reason of this #earthquake M...</td><td>1</td></tr><tr><td>1</td><td>4</td><td>NaN</td><td>NaN</td><td>Forest fire near La Ronge Sask. Canada</td><td>1</td></tr><tr><td>2</td><td>5</td><td>NaN</td><td>NaN</td><td>All residents asked to &#x27;shelter in place&#x27; are ...</td><td>1</td></tr><tr><td>3</td><td>6</td><td>NaN</td><td>NaN</td><td>13,000 people receive #wildfires evacuation or...</td><td>1</td></tr><tr><td>4</td><td>7</td><td>NaN</td><td>NaN</td><td>Just got sent this photo from Ruby #Alaska as ...</td><td>1</td></tr></tbody></table>
<pre><code class="language-python"># checking if the dataset is balanced
print(train_df.target.value_counts())
# 0    4342
# 1    3271

print(len(train_df), len(test_df))
# 7613 3263
</code></pre>
<pre><code class="language-python">train_df_shuffle = train_df.sample(frac=1, random_state=SEED)

train_df_shuffle.head(5)
</code></pre>
<table><thead><tr><th></th><th>id</th><th>keyword</th><th>location</th><th>text</th><th>target</th></tr></thead><tbody><tr><td>2644</td><td>3796</td><td>destruction</td><td>NaN</td><td>So you have a new weapon that can cause un-ima...</td><td>1</td></tr><tr><td>2227</td><td>3185</td><td>deluge</td><td>NaN</td><td>The f$&amp;@ing things I do for #GISHWHES Just...</td><td>0</td></tr><tr><td>5448</td><td>7769</td><td>police</td><td>UK</td><td>DT @georgegalloway: RT @Galloway4Mayor: The...</td><td>1</td></tr><tr><td>132</td><td>191</td><td>aftershock</td><td>NaN</td><td>Aftershock back to school kick off was great. ...</td><td>0</td></tr><tr><td>6845</td><td>9810</td><td>trauma</td><td>Montgomery County, MD</td><td>in response to trauma Children of Addicts deve...</td><td>0</td></tr></tbody></table>
<pre><code class="language-python">random_index = random.randint(0, len(train_df)-5)

for row in train_df_shuffle[[&#x27;text&#x27;, &#x27;target&#x27;]][random_index:random_index+5].itertuples():
    _, text, target = row
    print(f&quot;Target: {target}&quot;,&quot;(disaster)&quot; if target&gt;0 else &quot;(not a disaster)&quot;)
    print(f&quot;Text: {text}\n&quot;)
    print(&quot;---\n&quot;)
</code></pre>
<pre><code>Target: 1 (disaster)
Text: @WesleyLowery ?????? how are you going to survive this devastation?

---

Target: 0 (not a disaster)
Text: Hollywood movie about trapped miners released in Chile http://t.co/xe0EE1Fzfh

---

Target: 0 (not a disaster)
Text: #sing #tsunami Beginners #computer tutorial.: http://t.co/ukQYbhxMQI Everyone Wants To Learn To Build A Pc. Re http://t.co/iDWS2ZgYsa

---

Target: 1 (disaster)
Text: #Reddit updates #content #policy promises to quarantine extremely offensive communities http://t.co/EHGtZhKAn4

---

Target: 1 (disaster)
Text: Japan Marks 70th Anniversary of Hiroshima Atomic Bombing http://t.co/cQLM9jOJOP

---
</code></pre>
<h3 id="train-test-split">Train Test Split</h3>
<pre><code class="language-python">train_tweets, val_tweets, train_labels, val_labels = train_test_split(
    train_df_shuffle[&#x27;text&#x27;].to_numpy(),
    train_df_shuffle[&#x27;target&#x27;].to_numpy(),
    test_size=0.1,
    random_state=SEED)

print(len(train_tweets), len(val_tweets))
# 6851 762
</code></pre>
<h3 id="tokenization-and-embedding">Tokenization and Embedding</h3>
<p>Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to <a href="https://www.tensorflow.org/text/guide/word_embeddings">&quot;vectorize&quot; the text</a>) before feeding it to the model.</p>
<h4 id="tokenization">Tokenization</h4>
<p><a href="https://www.tensorflow.org/tutorials/keras/text_classification">TextVectorization layer</a> is a Keras layer to standardize, tokenize, and vectorize the dataset.</p>
<pre><code class="language-python"># find average number of words in tweets
average_tokens_per_tweet=round(sum([len(i.split()) for i in train_tweets])/len(train_tweets))
print(average_tokens_per_tweet)
# 15
</code></pre>
<pre><code class="language-python"># create text vectorizer
max_features = 10000 # limit to most common words
sequence_length = 15 # limit to average number of words in tweets

text_vectorizer = TextVectorization(
    max_tokens=max_features, # set a value to only include most common words
    standardize=&#x27;lower_and_strip_punctuation&#x27;,
    split=&#x27;whitespace&#x27;,
    ngrams=None, # set value to form common word groups
    output_mode=&#x27;int&#x27;,
    output_sequence_length=sequence_length, # set value to limit tweet size
    pad_to_max_tokens=True # fluff tweets that are shorter than set max length
)
</code></pre>
<pre><code class="language-python"># fit text vectorizer to training data
text_vectorizer.adapt(train_tweets)
</code></pre>
<pre><code class="language-python"># test fitted vectorizer
sample_sentence1 = &quot;Next I&#x27;m buying Coca-Cola to put the cocaine back in&quot;
sample_sentence2 = &quot;Hey guys, wanna feel old? I&#x27;m 40. You&#x27;re welcome.&quot;
sample_sentence3 = &quot;Beef chicken pork bacon chuck shortloin sirloin shank eu, bresaola voluptate in enim ea kielbasa laboris brisket laborum, jowl labore id porkchop elit ad commodo.&quot;
text_vectorizer([sample_sentence1,sample_sentence2,sample_sentence3])

# &lt;tf.Tensor: shape=(3, 15), dtype=int64, numpy=
# array([[ 274,   32, 4046,    1,    5,  370,    2, 5962,   88,    4,    0,
#            0,    0,    0,    0],
#        [ 706,  576,  473,  214,  206,   32,  354,  172, 1569,    0,    0,
#            0,    0,    0,    0],
#        [   1, 4013,    1,    1,    1,    1,    1,    1, 3878,    1,    1,
#            4,    1,    1,    1]])&gt;
</code></pre>
<pre><code class="language-python">random_tweet = random.choice(train_tweets)
vector = text_vectorizer([random_tweet])

print(
    f&#x27;Tweet: {random_tweet}\
    \n\nVector: {vector}&#x27;
)

# Tweet: Ignition Knock (Detonation) Sensor ACDelco GM Original Equipment 213-4678
# Vector: [[ 888  885  580 1767    1 1671 1623 1863    1    1    1    0    0    0
#      0]]
</code></pre>
<pre><code class="language-python"># get unique vocabulary
words_in_vocab = text_vectorizer.get_vocabulary()
top_10_words = words_in_vocab[:10]
bottom_10_words = words_in_vocab[-10:]

print(
    len(words_in_vocab),
    top_10_words,
    bottom_10_words
)
# 10000
# [&#x27;&#x27;, &#x27;[UNK]&#x27;, &#x27;the&#x27;, &#x27;a&#x27;, &#x27;in&#x27;, &#x27;to&#x27;, &#x27;of&#x27;, &#x27;and&#x27;, &#x27;i&#x27;, &#x27;is&#x27;]
# [&#x27;painthey&#x27;, &#x27;painful&#x27;, &#x27;paine&#x27;, &#x27;paging&#x27;, &#x27;pageshi&#x27;, &#x27;pages&#x27;, &#x27;paeds&#x27;, &#x27;pads&#x27;, &#x27;padres&#x27;, &#x27;paddytomlinson1&#x27;]

# The [UNK] stands for unknown - meaning outside of the 10.000 tokens limit
</code></pre>
<h4 id="embedding">Embedding</h4>
<p>The now vectorize data can be used as the first layer of the classification model, feeding transformed strings into an <a href="https://www.tensorflow.org/text/guide/word_embeddings">Embedding layer</a>. The Embedding layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array.</p>
<pre><code class="language-python"># create embedding layer
embedding = Embedding(
    input_dim = max_features,
    output_dim = 128,
    input_length = sequence_length
)
</code></pre>
<pre><code class="language-python">random_tweet = random.choice(train_tweets)
sample_embedd = embedding(text_vectorizer([random_tweet]))

print(
    f&#x27;Tweet: {random_tweet}\
    \n\nEmbedding: {sample_embedd}, {sample_embedd.shape}&#x27;
)

# Tweet: @biggangVH1 looks like George was having a panic attack. LOL.    

# Embedding: [[[-0.02811491 -0.02710991 -0.04273632 ...  0.01480064 -0.02413664
#     0.02612327]
#   [-0.013403   -0.04941868  0.03431542 ...  0.00432001 -0.03614474
#     0.04559914]
#   [ 0.00161045 -0.02501463  0.02461291 ... -0.02123032  0.02596099
#    -0.02626952]
#   ...
#   [ 0.03742747  0.03854593 -0.02052871 ...  0.01287705 -0.04228047
#    -0.02316147]
#   [ 0.03742747  0.03854593 -0.02052871 ...  0.01287705 -0.04228047
#    -0.02316147]
#   [ 0.03742747  0.03854593 -0.02052871 ...  0.01287705 -0.04228047
#    -0.02316147]]], (1, 15, 128)

# the shape tells us the the layer received 1 input with the length of
# 15 words (as set before and filled up with zeros if tweet is shorter)
# and each of those words is now represented by a 128dim vector
</code></pre>
<h2 id="experiments">Experiments</h2>
<ul>
<li>Model 0: Naive Bayes (baseline)</li>
<li>Model 1: Feed-forward neural network (dense model)</li>
<li>Model 2: LSTM model (RNN)</li>
<li>Model 3: GRU model (RNN)</li>
<li>Model 4: Bidirectional-LSTM model (RNN)</li>
<li>Model 5: 1D-Convolutional Neural Network (CNN)</li>
<li>Model 6: TensorFlow Hub pre-trained NLP feature extractor</li>
<li>Model 6a: Model 6 with added complexity (add dense layer)</li>
<li>Model 6b: Model 6a with 10% training data</li>
<li>Model 6c: Model 6a with 10% training data (fixed sampling)</li>
</ul>
<h3 id="model-0-naive-bayes-tf-ids">Model 0: Naive Bayes tf-ids</h3>
<h4 id="tokenization-and-modelling-pipeline">Tokenization and Modelling Pipeline</h4>
<pre><code class="language-python">model_0 = Pipeline([
    (&quot;tfidf&quot;, TfidfVectorizer()),
    (&quot;clf&quot;, MultinomialNB())
])

# fit to training data
model_0.fit(train_tweets, train_labels)
</code></pre>
<h4 id="evaluation">Evaluation</h4>
<pre><code class="language-python">baseline_score = model_0.score(val_tweets, val_labels)
print(f&quot;Baseline accuracy: {baseline_score*100:.2f}%&quot;)
# Baseline accuracy: 79.27%
</code></pre>
<h4 id="predictions">Predictions</h4>
<pre><code class="language-python">baseline_preds = model_0.predict(val_tweets)

print(val_tweets[:10])
print(baseline_preds[:10])
# array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0])
</code></pre>
<blockquote>
<p><code>[1 1 1 0 0 1 1 1 1 0]</code></p>
</blockquote>
<ol>
<li>&#x27;DFR EP016 Monthly Meltdown - On Dnbheaven 2015.08.06 <a href="http://t.co/EjKRf8N8A8">http://t.co/EjKRf8N8A8</a> #Drum and Bass #heavy #nasty <a href="http://t.co/SPHWE6wFI5">http://t.co/SPHWE6wFI5</a>&#x27;</li>
<li>&#x27;FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps <a href="http://t.co/qZQc8WWwcN">http://t.co/qZQc8WWwcN</a> via @usatoday&#x27;</li>
<li>&#x27;Gunmen kill four in El Salvador bus attack: Suspected Salvadoran gang members killed four people and wounded s... <a href="http://t.co/CNtwB6ScZj">http://t.co/CNtwB6ScZj</a>&#x27;</li>
<li>&#x27;@camilacabello97 Internally and externally screaming&#x27;</li>
<li>&#x27;Radiation emergency #preparedness starts with knowing to: get inside stay inside and stay tuned <a href="http://t.co/RFFPqBAz2F">http://t.co/RFFPqBAz2F</a> via @CDCgov&#x27;</li>
<li>&#x27;Investigators rule catastrophic structural failure resulted in 2014 Virg.. Related Articles: <a href="http://t.co/Cy1LFeNyV8">http://t.co/Cy1LFeNyV8</a>&#x27;</li>
<li>&#x27;How the West was burned: Thousands of wildfires ablaze in #California alone <a href="http://t.co/iCSjGZ9tE1">http://t.co/iCSjGZ9tE1</a> #climate #energy <a href="http://t.co/9FxmN0l0Bd">http://t.co/9FxmN0l0Bd</a>&#x27;</li>
<li>&quot;Map: Typhoon Soudelor&#x27;s predicted path as it approaches Taiwan; expected to make landfall over southern China by S\x89_ <a href="http://t.co/JDVSGVhlIs">http://t.co/JDVSGVhlIs</a>&quot;</li>
<li>&#x27;\x8993 blasts accused Yeda Yakub dies in Karachi of heart attack <a href="http://t.co/mfKqyxd8XG">http://t.co/mfKqyxd8XG</a> #Mumbai&#x27;</li>
<li>&#x27;My ears are bleeding  <a href="https://t.co/k5KnNwugwT">https://t.co/k5KnNwugwT</a>&#x27;]</li>
</ol>
<pre><code class="language-python">baseline_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=baseline_preds
)

print(f&quot;Accuracy: {baseline_metrics[&#x27;accuracy&#x27;]}, Precision: {baseline_metrics[&#x27;precision&#x27;]}, Recall: {baseline_metrics[&#x27;recall&#x27;]}, F1-Score: {baseline_metrics[&#x27;f1&#x27;]}&quot;)

</code></pre>
<h4 id="model-0-metrics">Model 0 Metrics</h4>
<ul>
<li>Accuracy: 79.26509186351706,</li>
<li>Precision: 0.8111390004213173,</li>
<li>Recall: 0.7926509186351706,</li>
<li>F1-Score: 0.7862189758049549</li>
</ul>
<h3 id="model-1-simple-dense">Model 1: Simple Dense</h3>
<h4 id="model-building-and-training">Model Building and Training</h4>
<pre><code class="language-python"># build model
## inputs are single 1-dimensional strings
inputs = Input(shape=(1,), dtype=tf.string)
## turn strings into numbers
x = text_vectorizer(inputs)
## create embedding from vectorized input
x = embedding(x)
## instead of returning a prediction for every token/word
## condense all to a single prediction for entire input string
x = GlobalAveragePooling1D()(x)
## sigmoid activated output for binary classification
outputs = Dense(1, activation=&#x27;sigmoid&#x27;)(x)

model_1 = tf.keras.Model(inputs, outputs, name=&#x27;model_1_dense&#x27;)
model_1.summary()
</code></pre>
<pre><code>Model: &quot;model_1_dense&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization (TextVec  (None, 15)               0         
 torization)                                                     
                                                                 
 embedding (Embedding)       (None, 15, 128)           1280000   
                                                                 
 global_average_pooling1d (G  (None, 128)              0         
 lobalAveragePooling1D)                                          
                                                                 
 dense_2 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,280,129
Trainable params: 1,280,129
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<pre><code class="language-python"># compile model
model_1.compile(
    loss=&#x27;binary_crossentropy&#x27;,
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&#x27;accuracy&#x27;]
)
</code></pre>
<pre><code class="language-python"># there seems to be an issue with the tb callback
# https://github.com/keras-team/keras/issues/15163
# changed histogram_freq=0
## create a callback to track experiments in TensorBoard
def create_tensorboard_callback_bugged(dir_name, experiment_name):
    # log progress to log directory
    log_dir = dir_name + &quot;/&quot; + experiment_name + &quot;/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)
    print(f&quot;INFO :: Saving TensorBoard Log to: {log_dir}&quot;)
    return tensorboard_callback
</code></pre>
<pre><code class="language-python"># model training
model_1_history = model_1.fit(
    x=train_tweets,
    y=train_labels,
    epochs=5,
    validation_data=(val_tweets, val_labels),
    callbacks=[create_tensorboard_callback_bugged(
        dir_name=LOG_DIR,
        experiment_name=&#x27;model_1_dense&#x27;
    )]
)
</code></pre>
<h4 id="model-evaluation">Model Evaluation</h4>
<pre><code class="language-python">model_1.evaluate(val_tweets, val_labels)
# loss: 0.4830 - accuracy: 0.7887
</code></pre>
<pre><code class="language-python">model_1_preds = model_1.predict(val_tweets)
sample_prediction=model_1_preds[0]
print(f&quot;Prediction: {sample_prediction}&quot;,&quot;(disaster)&quot; if sample_prediction&gt;0.5 else &quot;(not a disaster)&quot;)
# Prediction: [0.32197773] (not a disaster)
</code></pre>
<pre><code class="language-python"># convert model prediction probabilities to binary label format
model_1_preds = tf.squeeze(tf.round(model_1_preds))
</code></pre>
<pre><code class="language-python">model_1_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=model_1_preds
)

print(model_1_metrics)
</code></pre>
<h4 id="model-1-metrics">Model 1 Metrics</h4>
<ul>
<li>Accuracy: 78.87139107611549</li>
<li>Precision: 0.7969619064252174</li>
<li>Recall: 0.7887139107611548</li>
<li>F1: 0.7847294282013199</li>
</ul>
<pre><code class="language-python"># the model performs worse than the baseline model
np.array(list(model_1_metrics.values())) &gt; np.array(list(baseline_metrics.values()))
# array([False, False, False, False])
</code></pre>
<h4 id="visualize-the-embedding">Visualize the Embedding</h4>
<pre><code class="language-python"># get vocab from text vectorizer layer
words_in_vocab = text_vectorizer.get_vocabulary()
top_10_words = words_in_vocab[:10]
print(top_10_words)
# [&#x27;&#x27;, &#x27;[UNK]&#x27;, &#x27;the&#x27;, &#x27;a&#x27;, &#x27;in&#x27;, &#x27;to&#x27;, &#x27;of&#x27;, &#x27;and&#x27;, &#x27;i&#x27;, &#x27;is&#x27;]
</code></pre>
<pre><code class="language-python"># get embedding weight matrix
embed_weights = model_1.get_layer(&#x27;embedding&#x27;).get_weights()[0]
print(embed_weights.shape)
# (10000, 128) same size as max_features - one weight for every word in vocabulary
</code></pre>
<p>The <a href="https://www.tensorflow.org/tutorials/text/word2vec">embeddeding weights</a> started as random numbers assigned to each token/word in our dataset. By fitting this embedding space to our dataset these weights can now be used to group the words in our dataset. Words that belong to the same class should also have similar vectors representing them.</p>
<p>We can use the <a href="https://projector.tensorflow.org/">Tensorflow Projector</a> to display our embedding space:</p>
<pre><code class="language-python">out_v = io.open(&#x27;embedding_weights/vectors_model1.tsv&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;)
out_m = io.open(&#x27;embedding_weights/metadata_model1.tsv&#x27;, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;)

for index, word in enumerate(words_in_vocab):
    if index == 0:
        continue # skip padding
    vec = embed_weights[index]
    out_v.write(&#x27;\t&#x27;.join([str(x) for x in vec]) + &#x27;\n&#x27;)
    out_m.write(word + &#x27;\n&#x27;)
    
out_v.close()
out_m.close()

# Upload both files to the [Tensorflow Projector](https://projector.tensorflow.org/)
</code></pre>
<p><img alt="(Re) Introduction to Tensorflow Natural Language Processing" src="/assets/images/tf_nlp_desaster_tweets_01-9df7623eb1b424d4d16ff3f6b9bd98d4.png" width="866" height="713"></p>
<p>The projection shows a clear separation between our two classes showing each word in our vocabulary as a member of one of two clusters.</p>
<h3 id="model-2-lstm-long-term-short-term-memory-rnn">Model 2: LSTM Long-term Short-term Memory RNN</h3>
<h4 id="model-building-and-training-1">Model Building and Training</h4>
<pre><code class="language-python">inputs = Input(shape=(1,), dtype=&#x27;string&#x27;)
x = text_vectorizer(inputs)
x = embedding(x)
# x = LSTM(64, return_sequences=True)(x)
x = LSTM(64)(x)
x = Dense(64, activation=&#x27;relu&#x27;)(x)
outputs = Dense(1, activation=&#x27;sigmoid&#x27;)(x)

model_2 = tf.keras.Model(inputs, outputs, name=&quot;model_2_LSTM&quot;)
model_2.summary()
</code></pre>
<pre><code>Model: &quot;model_2_LSTM&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization (TextVec  (None, 15)               0         
 torization)                                                     
                                                                 
 embedding (Embedding)       (None, 15, 128)           1280000   
                                                                 
 lstm_2 (LSTM)               (None, 64)                49408     
                                                                 
 dense_3 (Dense)             (None, 64)                4160      
                                                                 
 dense_4 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 1,333,633
Trainable params: 1,333,633
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<pre><code class="language-python">model_2.compile(
    loss=&#x27;binary_crossentropy&#x27;,
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&#x27;accuracy&#x27;]
)
</code></pre>
<pre><code class="language-python">model_2_history = model_2.fit(
    train_tweets,
    train_labels,
    epochs=5,
    validation_data=(val_tweets, val_labels),
    callbacks=[create_tensorboard_callback_bugged(
        dir_name=LOG_DIR,
        experiment_name=&#x27;model_2_lstm&#x27;
    )]
)
</code></pre>
<h4 id="model-evaluation-1">Model Evaluation</h4>
<pre><code class="language-python">model_2.evaluate(val_tweets, val_labels)
# loss: 1.6905 - accuracy: 0.7743
</code></pre>
<pre><code class="language-python"># make predictions
model_2_preds = model_2.predict(val_tweets)
sample_prediction=model_2_preds[0]
print(f&quot;Prediction: {sample_prediction}&quot;,&quot;(disaster)&quot; if sample_prediction&gt;0.5 else &quot;(not a disaster)&quot;)
# Prediction: [0.04310093] (not a disaster)
</code></pre>
<pre><code class="language-python"># convert model prediction probabilities to binary label format
model_2_preds = tf.squeeze(tf.round(model_2_preds))
</code></pre>
<pre><code class="language-python">model_2_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=model_2_preds
)

print(model_2_metrics)
</code></pre>
<h4 id="model-2-metrics">Model 2 Metrics</h4>
<ul>
<li>Accuracy: 76.77165354330708</li>
<li>Precision: 0.7674723453090632</li>
<li>Recall: 0.7677165354330708</li>
<li>F1: 0.7668863186407149</li>
</ul>
<pre><code class="language-python"># the model performs worse than the previous model
np.array(list(model_2_metrics.values())) &gt; np.array(list(model_1_metrics.values()))
# array([False, False, False, False])
</code></pre>
<h3 id="model-3-gru-gated-recurrent-unit-rnn">Model 3: GRU Gated Recurrent Unit RNN</h3>
<h4 id="model-building-and-training-2">Model Building and Training</h4>
<pre><code class="language-python">inputs = Input(shape=(1,), dtype=tf.string)
x = text_vectorizer(inputs)
x = embedding(x)
# x = GRU(64, return_sequences=True)(x)
x = GRU(64)(x)
x = Dense(64, activation=&#x27;relu&#x27;)(x)
outputs = Dense(1, activation=&#x27;sigmoid&#x27;)(x)

model_3 = tf.keras.Model(inputs, outputs, name=&quot;model_3_GRU&quot;)
model_3.summary()
</code></pre>
<pre><code>Model: &quot;model_3_GRU&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization (TextVec  (None, 15)               0         
 torization)                                                     
                                                                 
 embedding (Embedding)       (None, 15, 128)           1280000   
                                                                 
 gru (GRU)                   (None, 64)                37248     
                                                                 
 dense_5 (Dense)             (None, 64)                4160      
                                                                 
 dense_6 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 1,321,473
Trainable params: 1,321,473
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<pre><code class="language-python">model_3.compile(
    loss=&#x27;binary_crossentropy&#x27;,
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&#x27;accuracy&#x27;]
)
</code></pre>
<pre><code class="language-python">model_3_history = model_3.fit(
    train_tweets,
    train_labels,
    epochs=5,
    validation_data=(val_tweets, val_labels),
    callbacks=[create_tensorboard_callback_bugged(
        dir_name=LOG_DIR,
        experiment_name=&#x27;model_3_gru&#x27;
    )]
)
</code></pre>
<h4 id="model-evaluation-2">Model Evaluation</h4>
<pre><code class="language-python">model_3.evaluate(val_tweets, val_labels)
# loss: 1.2467 - accuracy: 0.7703
</code></pre>
<pre><code class="language-python"># make predictions
model_3_preds = model_3.predict(val_tweets)
sample_prediction=model_3_preds[0]
print(f&quot;Prediction: {sample_prediction}&quot;,&quot;(disaster)&quot; if sample_prediction&gt;0.5 else &quot;(not a disaster)&quot;)
# Prediction: [0.00013416] (not a disaster)
</code></pre>
<pre><code class="language-python"># convert model prediction probabilities to binary label format
model_3_preds = tf.squeeze(tf.round(model_3_preds))
</code></pre>
<pre><code class="language-python">model_3_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=model_3_preds
)

print(model_3_metrics)
</code></pre>
<h4 id="model-3-metrics">Model 3 Metrics</h4>
<ul>
<li>Accuracy: 76.9028871391076</li>
<li>Precision: 0.7768747910576218</li>
<li>Recall: 0.7690288713910761</li>
<li>F1: 0.7643954892702002</li>
</ul>
<pre><code class="language-python"># the model performs (sometimes) better than the lstm but still worse than the baseline model
print(np.array(list(model_3_metrics.values())) &gt; np.array(list(model_2_metrics.values())))
# array([ True,  True,  True, False])
print(np.array(list(model_3_metrics.values())) &gt; np.array(list(baseline_metrics.values())))
# array([False, False, False, False])
</code></pre>
<h3 id="model-4-bi-directional-rnn">Model 4: Bi-Directional RNN</h3>
<h4 id="model-building-and-training-3">Model Building and Training</h4>
<pre><code class="language-python">inputs = Input(shape=(1,), dtype=tf.string)
x = text_vectorizer(inputs)
x = embedding(x)
x = Bidirectional(LSTM(64, return_sequences=True))(x)
x = Bidirectional(GRU(64))(x)
outputs = Dense(1, activation=&#x27;sigmoid&#x27;)(x)

model_4 = tf.keras.Model(inputs, outputs, name=&#x27;model_4_bidirectional&#x27;)
model_4.summary()
</code></pre>
<pre><code>Model: &quot;model_4_bidirectional&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization (TextVec  (None, 15)               0         
 torization)                                                     
                                                                 
 embedding (Embedding)       (None, 15, 128)           1280000   
                                                                 
 bidirectional_2 (Bidirectio  (None, 15, 128)          98816     
 nal)                                                            
                                                                 
 bidirectional_3 (Bidirectio  (None, 128)              74496     
 nal)                                                            
                                                                 
 dense_8 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 1,453,441
Trainable params: 1,453,441
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<pre><code class="language-python">model_4.compile(
    loss=&#x27;binary_crossentropy&#x27;,
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&#x27;accuracy&#x27;]
)
</code></pre>
<pre><code class="language-python">model_4_history = model_4.fit(
    train_tweets,
    train_labels,
    epochs=5,
    validation_data=(val_tweets, val_labels),
    callbacks=[create_tensorboard_callback_bugged(
        dir_name=LOG_DIR,
        experiment_name=&#x27;model_4_bidirectional&#x27;
    )]
)
</code></pre>
<h4 id="model-evaluation-3">Model Evaluation</h4>
<pre><code class="language-python">model_4.evaluate(val_tweets, val_labels)
# loss: 1.7367 - accuracy: 0.7756
</code></pre>
<pre><code class="language-python"># make predictions
model_4_preds = model_4.predict(val_tweets)
sample_prediction=model_4_preds[0]
print(f&quot;Prediction: {sample_prediction}&quot;,&quot;(disaster)&quot; if sample_prediction&gt;0.5 else &quot;(not a disaster)&quot;)
# Prediction: [0.00025736] (not a disaster)
</code></pre>
<pre><code class="language-python"># convert model prediction probabilities to binary label format
model_4_preds = tf.squeeze(tf.round(model_4_preds))
</code></pre>
<pre><code class="language-python">model_4_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=model_4_preds
)

print(model_4_metrics)
</code></pre>
<h4 id="model-4-metrics">Model 4 Metrics</h4>
<ul>
<li>Accuracy: 77.55905511811024</li>
<li>Precision: 0.7777490986405654</li>
<li>Recall: 0.7755905511811023</li>
<li>F1: 0.7733619560087615</li>
</ul>
<pre><code class="language-python"># the model performs better than the gru but worse than the baseline model
print(np.array(list(model_4_metrics.values())) &gt; np.array(list(model_3_metrics.values())))
# [ True  True  True  True]
print(np.array(list(model_4_metrics.values())) &gt; np.array(list(baseline_metrics.values())))
# [False False False False]
</code></pre>
<h3 id="model-5-conv1d">Model 5: Conv1D</h3>
<h4 id="model-building-and-training-4">Model Building and Training</h4>
<pre><code class="language-python">inputs = Input(shape=(1,), dtype=tf.string)
x = text_vectorizer(inputs)
x = embedding(x)
x = Conv1D(
    filters=64,
    kernel_size=5, # check 5 words at a time
    activation=&#x27;relu&#x27;,
    padding=&#x27;valid&#x27;,
    strides=1
)(x)
x = GlobalMaxPool1D()(x)
outputs = Dense(1, activation=&#x27;sigmoid&#x27;)(x)

model_5 = tf.keras.Model(inputs, outputs, name=&#x27;model_5_conv1d&#x27;)
model_5.summary()
</code></pre>
<pre><code>Model: &quot;model_5_conv1d&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_14 (InputLayer)       [(None, 1)]               0         
                                                                 
 text_vectorization (TextVec  (None, 15)               0         
 torization)                                                     
                                                                 
 embedding (Embedding)       (None, 15, 128)           1280000   
                                                                 
 conv1d_6 (Conv1D)           (None, 11, 64)            41024     
                                                                 
 global_max_pooling1d_5 (Glo  (None, 64)               0         
 balMaxPooling1D)                                                
                                                                 
 dense_14 (Dense)            (None, 1)                 65        
                                                                 
=================================================================
Total params: 1,321,089
Trainable params: 1,321,089
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<pre><code class="language-python">model_5.compile(
    loss=&#x27;binary_crossentropy&#x27;,
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&#x27;accuracy&#x27;]
)
</code></pre>
<pre><code class="language-python">model_5_history = model_5.fit(
    train_tweets,
    train_labels,
    epochs=5,
    validation_data=(val_tweets, val_labels),
    callbacks=[create_tensorboard_callback_bugged(
        dir_name=LOG_DIR,
        experiment_name=&#x27;model_5_conv1d&#x27;
    )]
)
</code></pre>
<h4 id="model-evaluation-4">Model Evaluation</h4>
<pre><code class="language-python">model_5.evaluate(val_tweets, val_labels)
# loss: 1.3018 - accuracy: 0.7454
</code></pre>
<pre><code class="language-python"># make predictions
model_5_preds = model_5.predict(val_tweets)
sample_prediction=model_5_preds[0]
print(f&quot;Prediction: {sample_prediction}&quot;,&quot;(disaster)&quot; if sample_prediction&gt;0.5 else &quot;(not a disaster)&quot;)
# Prediction: [0.05387583] (not a disaster)
</code></pre>
<pre><code class="language-python"># convert model prediction probabilities to binary label format
model_5_preds = tf.squeeze(tf.round(model_5_preds))
</code></pre>
<pre><code class="language-python">model_5_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=model_5_preds
)

print(model_5_metrics)
</code></pre>
<h4 id="model-5-metrics">Model 5 Metrics</h4>
<ul>
<li>Accuracy: 74.67191601049869</li>
<li>Precision: 0.7465631385096111</li>
<li>Recall: 0.7467191601049868</li>
<li>F1: 0.7453858813570734</li>
</ul>
<pre><code class="language-python"># the model performs worse than the gru and worse than the baseline model
print(np.array(list(model_5_metrics.values())) &gt; np.array(list(model_4_metrics.values())))
# [False False False False]
print(np.array(list(model_5_metrics.values())) &gt; np.array(list(baseline_metrics.values())))
# [False False False False]
</code></pre>
<h3 id="model-6-transfer-learning-feature-extractor">Model 6: Transfer Learning Feature Extractor</h3>
<p><strong>USE</strong> Feature Extractor - <a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder</a></p>
<pre><code class="language-python"># https://tfhub.dev/google/collections/universal-sentence-encoder/1
embed = hub.load(&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;)
</code></pre>
<pre><code class="language-python"># test the encoder
sample_sentence1 = &quot;Next I&#x27;m buying Coca-Cola to put the cocaine back in&quot;
sample_sentence2 = &quot;Hey guys, wanna feel old? I&#x27;m 40. You&#x27;re welcome.&quot;
sample_sentence3 = &quot;Beef chicken pork bacon chuck shortloin sirloin shank eu, bresaola voluptate in enim ea kielbasa laboris brisket laborum, jowl labore id porkchop elit ad commodo.&quot;

embed_sample = embed([
    sample_sentence1,
    sample_sentence2,
    sample_sentence3
])

print(embed_sample)

# the encoder turns each input into size-512 feature vectors
# [[ 0.06844153 -0.0325974  -0.01901028 ... -0.03307429 -0.04625704
#  -0.08149158]
# [ 0.02810995 -0.06714624  0.02414106 ... -0.02519046  0.03197665
#   0.02462349]
# [ 0.03188843 -0.0167392  -0.03194157 ... -0.04541751 -0.05822486
#  -0.07237621]], shape=(3, 512), dtype=float32)
</code></pre>
<h4 id="model-building-and-training-5">Model Building and Training</h4>
<pre><code class="language-python">model_6 = tf.keras.models.Sequential(name=&#x27;model_6_use&#x27;)

model_6.add(hub.KerasLayer(
    &#x27;https://tfhub.dev/google/universal-sentence-encoder/4&#x27;, 
    input_shape=[], # layer excepts string of variable length and returns a 512 feature vector
    dtype=tf.string, 
    trainable=False,
    name=&#x27;sentence-encoder&#x27;
))

model_6.add(Dense(1, activation=&#x27;sigmoid&#x27;))

model_6.summary()
</code></pre>
<pre><code>Model: &quot;model_6_use&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 sentence-encoder (KerasLaye  (None, 512)              256797824 
 r)                                                              
                                                                 
 dense_5 (Dense)             (None, 1)                 513       
                                                                 
=================================================================
Total params: 256,798,337
Trainable params: 513
Non-trainable params: 256,797,824
_________________________________________________________________
</code></pre>
<pre><code class="language-python">model_6.compile(
    loss=&#x27;binary_crossentropy&#x27;,
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&#x27;accuracy&#x27;]
)
</code></pre>
<pre><code class="language-python">model_6_history = model_6.fit(
    train_tweets,
    train_labels,
    epochs=5,
    validation_data=(val_tweets, val_labels),
    callbacks=[create_tensorboard_callback_bugged(
        dir_name=LOG_DIR,
        experiment_name=&#x27;model_6_use&#x27;
    )]
)
</code></pre>
<h4 id="model-evaluation-5">Model Evaluation</h4>
<pre><code class="language-python">model_6.evaluate(val_tweets, val_labels)
# loss: 0.4982 - accuracy: 0.7887
</code></pre>
<pre><code class="language-python"># make predictions
model_6_preds = model_6.predict(val_tweets)
sample_prediction=model_6_preds[0]
print(f&quot;Prediction: {sample_prediction}&quot;,&quot;(disaster)&quot; if sample_prediction&gt;0.5 else &quot;(not a disaster)&quot;)
# Prediction: [0.36132362] (not a disaster)
</code></pre>
<pre><code class="language-python"># convert model prediction probabilities to binary label format
model_6_preds = tf.squeeze(tf.round(model_6_preds))
</code></pre>
<pre><code class="language-python">model_6_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=model_6_preds
)

print(model_6_metrics)
</code></pre>
<h4 id="model-6-metrics">Model 6 Metrics</h4>
<ul>
<li>Accuracy: 78.87139107611549</li>
<li>Precision: 0.7891485217486439</li>
<li>Recall: 0.7887139107611548</li>
<li>F1: 0.7876016937745534</li>
</ul>
<pre><code class="language-python"># the model performs better than the conv1d and gettin close to the baseline model
print(np.array(list(model_6_metrics.values())) &gt; np.array(list(model_5_metrics.values())))
# [ True  True  True  True]
print(np.array(list(model_6_metrics.values())) &gt; np.array(list(baseline_metrics.values())))
# [False False False  True]
</code></pre>
<h3 id="model-6a-added-dense-layer">Model 6a (added Dense Layer)</h3>
<p>The model get&#x27;s close to the baseline model - I will try to add another Dense layer and see if this improves the performance.</p>
<p>Otherwise the model is identical - only written for functional API (no reason, just for practise :) )</p>
<pre><code class="language-python">inputs = Input(shape=[], dtype=tf.string)

x = hub.KerasLayer(
    &#x27;https://tfhub.dev/google/universal-sentence-encoder/4&#x27;, 
    trainable=False,
    dtype=tf.string,
    name=&#x27;sentence-encoder&#x27;
)(inputs)

x = Dense(64, activation=&#x27;relu&#x27;)(x)
output = Dense(1, activation=&#x27;sigmoid&#x27;)(x)

model_6a = tf.keras.models.Model(inputs, output, name=&#x27;model_6_use&#x27;)
model_6a.summary()
</code></pre>
<pre><code>Model: &quot;model_6_use&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None,)]                 0         
                                                                 
 sentence-encoder (KerasLaye  (None, 512)              256797824 
 r)                                                              
                                                                 
 dense_13 (Dense)            (None, 64)                32832     
                                                                 
 dense_14 (Dense)            (None, 1)                 65        
                                                                 
=================================================================
Total params: 256,830,721
Trainable params: 32,897
Non-trainable params: 256,797,824
_________________________________________________________________
</code></pre>
<pre><code class="language-python">model_6a.compile(
    loss=&#x27;binary_crossentropy&#x27;,
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&#x27;accuracy&#x27;]
)
</code></pre>
<pre><code class="language-python">model_6a_history = model_6a.fit(
    train_tweets,
    train_labels,
    epochs=5,
    validation_data=(val_tweets, val_labels),
    callbacks=[create_tensorboard_callback_bugged(
        dir_name=LOG_DIR,
        experiment_name=&#x27;model_6a_use&#x27;
    )]
)
</code></pre>
<h4 id="model-evaluation-6">Model Evaluation</h4>
<pre><code class="language-python">model_6a.evaluate(val_tweets, val_labels)
# loss: 0.4305 - accuracy: 0.8123
</code></pre>
<pre><code class="language-python"># make predictions
model_6a_preds = model_6a.predict(val_tweets)
sample_prediction=model_6a_preds[0]
print(f&quot;Prediction: {sample_prediction}&quot;,&quot;(disaster)&quot; if sample_prediction&gt;0.5 else &quot;(not a disaster)&quot;)
# Prediction: [0.15243901] (not a disaster)
</code></pre>
<pre><code class="language-python"># convert model prediction probabilities to binary label format
model_6a_preds = tf.squeeze(tf.round(model_6a_preds))
</code></pre>
<pre><code class="language-python">model_6a_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=model_6a_preds
)

print(model_6a_metrics)
</code></pre>
<h4 id="model-6a-metrics">Model 6a Metrics</h4>
<ul>
<li>Accuracy: 81.23359580052494</li>
<li>Precision: 0.8148798668657973</li>
<li>Recall: 0.8123359580052494</li>
<li>F1: 0.810686575717776</li>
</ul>
<pre><code class="language-python"># the model performs better than the conv1d AND better than the baseline model~!
print(np.array(list(model_6a_metrics.values())) &gt; np.array(list(model_5_metrics.values())))
# [ True  True  True  True]
print(np.array(list(model_6a_metrics.values())) &gt; np.array(list(baseline_metrics.values())))
# [ True  True  True  True]
</code></pre>
<h3 id="model-6b-transfer-learning-feature-extractor-10-dataset">Model 6b: Transfer Learning Feature Extractor (10% Dataset)</h3>
<h4 id="dataset-1">Dataset</h4>
<pre><code class="language-python"># create data subset with 10% of the training data
train_10_percent = train_df_shuffle[[&#x27;text&#x27;, &#x27;target&#x27;]].sample(frac=0.1, random_state=SEED)
print(len(train_df_shuffle), len(train_10_percent))
# 7613 761
</code></pre>
<pre><code class="language-python">train_tweets_10_percent = train_10_percent[&#x27;text&#x27;].to_list()
train_labels_10_percent = train_10_percent[&#x27;target&#x27;].to_list()
</code></pre>
<pre><code class="language-python"># check label distribution in randomized subset
dist_full_dataset = train_df_shuffle[&#x27;target&#x27;].value_counts()
dist_10_percent_subset = train_10_percent[&#x27;target&#x27;].value_counts()

print(
    (dist_full_dataset[0]/dist_full_dataset[1]),
    (dist_10_percent_subset[0]/dist_10_percent_subset[1])
)

# the full dataset has 33% more &quot;no-desaster&quot; tweets
# in the subset the overhang is lower with 19%
# 1.3274228064811984 1.1867816091954022
</code></pre>
<h4 id="model-building-and-training-6">Model Building and Training</h4>
<pre><code class="language-python">model_6b = tf.keras.models.Sequential(name=&#x27;model_6b_use_10_percent&#x27;)

model_6b.add(hub.KerasLayer(
    &#x27;https://tfhub.dev/google/universal-sentence-encoder/4&#x27;, 
    input_shape=[], # layer excepts string of variable length and returns a 512 feature vector
    dtype=tf.string, 
    trainable=False,
    name=&#x27;sentence-encoder&#x27;
))

model_6b.add(Dense(64, activation=&#x27;relu&#x27;))
model_6b.add(Dense(1, activation=&#x27;sigmoid&#x27;))

model_6b.summary()
</code></pre>
<pre><code>Model: &quot;model_6b_use_10_percent&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 sentence-encoder (KerasLaye  (None, 512)              256797824 
 r)                                                              
                                                                 
 dense_10 (Dense)            (None, 64)                32832     
                                                                 
 dense_11 (Dense)            (None, 1)                 65        
                                                                 
=================================================================
Total params: 256,830,721
Trainable params: 32,897
Non-trainable params: 256,797,824
_________________________________________________________________
</code></pre>
<pre><code class="language-python">model_6b.compile(
    loss=&#x27;binary_crossentropy&#x27;,
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&#x27;accuracy&#x27;]
)
</code></pre>
<pre><code class="language-python">model_6b_history = model_6b.fit(
    train_tweets_10_percent,
    train_labels_10_percent,
    epochs=5,
    validation_data=(val_tweets, val_labels),
    callbacks=[create_tensorboard_callback_bugged(
        dir_name=LOG_DIR,
        experiment_name=&#x27;model_6b_use&#x27;
    )]
)
</code></pre>
<h4 id="model-evaluation-7">Model Evaluation</h4>
<pre><code class="language-python">model_6b.evaluate(val_tweets, val_labels)
# loss: 0.3375 - accuracy: 0.8675
</code></pre>
<pre><code class="language-python"># make predictions
model_6b_preds = model_6b.predict(val_tweets)
sample_prediction=model_6b_preds[0]
print(f&quot;Prediction: {sample_prediction}&quot;,&quot;(disaster)&quot; if sample_prediction&gt;0.5 else &quot;(not a disaster)&quot;)
# Prediction: [0.16887127] (not a disaster)
</code></pre>
<pre><code class="language-python"># convert model prediction probabilities to binary label format
model_6b_preds = tf.squeeze(tf.round(model_6b_preds))
</code></pre>
<pre><code class="language-python">model_6b_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=model_6b_preds
)

print(model_6b_metrics)
</code></pre>
<h4 id="model-6b-metrics">Model 6b Metrics</h4>
<ul>
<li>Accuracy: 86.74540682414698</li>
<li>Precision: 0.8695676801900293</li>
<li>Recall: 0.8674540682414699</li>
<li>F1: 0.8666326892977956</li>
</ul>
<pre><code class="language-python"># the model performs even better with only 10% AND better than the baseline model ?
print(np.array(list(model_6b_metrics.values())) &gt; np.array(list(model_6_metrics.values())))
# [ True  True  True  True]
print(np.array(list(model_6b_metrics.values())) &gt; np.array(list(baseline_metrics.values())))
# [ True  True  True  True]
</code></pre>
<h3 id="model-6c-use-data-leakage-issue-10-dataset">Model 6c: USE Data Leakage Issue (10% Dataset)</h3>
<h4 id="data-leakage-issue">Data Leakage Issue</h4>
<p>Both the random 10% subset and the validation data was taken from the shuffled data <code>train_df_shuffle</code>. This means that the new training dataset might contain a small amount of data entries that are also part of the validation set. Since we are now only working with 10% of the original dataset this data leakage can affect our training massively.</p>
<p>The model needs to be retrained with a clean training dataset...</p>
<h5 id="training-10-split">Training 10% Split</h5>
<pre><code class="language-python"># create data subset with 10% of the training data
train_10_percent_split = int(0.1 * len(train_tweets))
train_tweets_10_percent_clean = train_tweets[:train_10_percent_split]
train_labels_10_percent_clean = train_labels[:train_10_percent_split]
print(len(train_tweets), len(train_tweets_10_percent_clean))
# 6851 685
</code></pre>
<pre><code class="language-python"># check label distribution in randomized subset
dist_train_dataset = pd.Series(train_labels).value_counts()
dist_10_percent_train_subset = pd.Series(train_labels_10_percent_clean).value_counts()


print(dist_train_dataset, dist_10_percent_train_subset)
# 0    3928
# 1    2923
# ---------
# 406
# 279

print(
    (dist_train_dataset[0]/dist_train_dataset[1]),
    (dist_10_percent_train_subset[0]/dist_10_percent_train_subset[1])
)

# the full dataset has 34% more &quot;no-desaster&quot; tweets
# in the subset the overhang is lower with 46% - not ideal
# 1.3438248374957236 1.4551971326164874
</code></pre>
<pre><code class="language-python"># use the same model as before but with cleaned weights
model_6c = tf.keras.models.clone_model(model_6b)

model_6c.compile(
    loss=&#x27;binary_crossentropy&#x27;,
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&#x27;accuracy&#x27;]
)
</code></pre>
<pre><code class="language-python">model_6c_history = model_6c.fit(
    train_tweets_10_percent_clean,
    train_labels_10_percent_clean,
    epochs=5,
    validation_data=(val_tweets, val_labels),
    callbacks=[create_tensorboard_callback_bugged(
        dir_name=LOG_DIR,
        experiment_name=&#x27;model_6c_use&#x27;
    )]
)
</code></pre>
<h4 id="model-evaluation-8">Model Evaluation</h4>
<pre><code class="language-python">model_6b.evaluate(val_tweets, val_labels)
# evaluation with data leakage - performance suspiciously high
# loss: 0.3375 - accuracy: 0.8675
model_6c.evaluate(val_tweets, val_labels)
# the model trained on the cleaned-up 10% dataset - as expected -
# performs worse than the model trained on the full dataset
# loss: 0.4904 - accuracy: 0.7822
</code></pre>
<pre><code class="language-python"># make predictions
model_6c_preds = model_6c.predict(val_tweets)
sample_prediction=model_6c_preds[0]
print(f&quot;Prediction: {sample_prediction}&quot;,&quot;(disaster)&quot; if sample_prediction&gt;0.5 else &quot;(not a disaster)&quot;)
# Prediction: [0.22155239] (not a disaster)
</code></pre>
<pre><code class="language-python"># convert model prediction probabilities to binary label format
model_6c_preds = tf.squeeze(tf.round(model_6c_preds))
</code></pre>
<pre><code class="language-python">model_6c_metrics = calculate_metrics(
    y_true=val_labels,
    y_pred=model_6c_preds
)

print(model_6c_metrics)
</code></pre>
<h4 id="model-6c-metrics">Model 6c Metrics</h4>
<ul>
<li>Accuracy: 78.21522309711287</li>
<li>Precision: 0.7827333187650908</li>
<li>Recall: 0.7821522309711286</li>
<li>F1: 0.7808542546238821</li>
</ul>
<pre><code class="language-python"># the model performs worse than the baseline but gets close
# considering that we only used 10% of the data
print(np.array(list(model_6c_metrics.values())) &gt; np.array(list(model_6b_metrics.values())))
# [False False False False]
print(np.array(list(model_6c_metrics.values())) &gt; np.array(list(baseline_metrics.values())))
# [False False False False]
</code></pre>
<h2 id="compare-experiments">Compare Experiments</h2>
<pre><code class="language-python"># combine training results in dataframe
model_metrics_all = pd.DataFrame({
    &quot;0_baseline&quot;: baseline_metrics,
    &quot;1_simple_dense&quot;: model_1_metrics,
    &quot;2_lstm&quot;: model_2_metrics,
    &quot;3_gru&quot;: model_3_metrics,
    &quot;4_bidirectional&quot;: model_4_metrics,
    &quot;5_conv1d&quot;: model_5_metrics,
    &quot;6_use&quot;: model_6_metrics,
    &quot;6a_use_added_dense&quot;: model_6a_metrics,
    &quot;6b_use_10_percent&quot;: model_6b_metrics,
    &quot;6c_use_10_percent_cleaned&quot;: model_6c_metrics
})

# transpose
model_metrics_all_transposed = model_metrics_all.transpose()

# rescale accuracy
model_metrics_all_transposed[&#x27;accuracy&#x27;] = model_metrics_all_transposed[&#x27;accuracy&#x27;]/100

# sort by F1-score
model_metrics_all_transposed_sorted = model_metrics_all_transposed.sort_values(by=[&#x27;f1&#x27;], ascending=False)
model_metrics_all_transposed_sorted
# The best performing 6b_use_10_percent is faulty
# The next best is the USE with 100% of training data 6a_use_added_dense
# The USE with 10% training data performs closely to the baseline and simple dense model
</code></pre>
<table><thead><tr><th></th><th>accuracy</th><th>precision</th><th>recall</th><th>f1</th></tr></thead><tbody><tr><td>6b_use_10_percent</td><td>0.867454</td><td>0.869568</td><td>0.867454</td><td>0.866633</td></tr><tr><td>6a_use_added_dense</td><td>0.813648</td><td>0.814169</td><td>0.813648</td><td>0.812791</td></tr><tr><td>0_baseline</td><td>0.792651</td><td>0.811139</td><td>0.792651</td><td>0.786219</td></tr><tr><td>1_simple_dense</td><td>0.788714</td><td>0.796962</td><td>0.788714</td><td>0.784729</td></tr><tr><td>6_use</td><td>0.784777</td><td>0.785083</td><td>0.784777</td><td>0.783692</td></tr><tr><td>6c_use_10_percent_cleaned</td><td>0.782152</td><td>0.782733</td><td>0.782152</td><td>0.780854</td></tr><tr><td>2_lstm</td><td>0.775591</td><td>0.776962</td><td>0.775591</td><td>0.773741</td></tr><tr><td>3_gru</td><td>0.770341</td><td>0.773598</td><td>0.770341</td><td>0.767494</td></tr><tr><td>4_bidirectional</td><td>0.765092</td><td>0.764730</td><td>0.765092</td><td>0.764430</td></tr><tr><td>5_conv1d</td><td>0.759843</td><td>0.759995</td><td>0.759843</td><td>0.758468</td></tr></tbody></table>
<pre><code class="language-python"># plot model results
model_metrics_all_transposed_sorted.plot(
    kind=&#x27;bar&#x27;,
    rot=30,
    figsize=(12,8)).legend(bbox_to_anchor=(1.0, 1.0)
)
</code></pre>
<p><img alt="(Re) Introduction to Tensorflow Natural Language Processing" src="/assets/images/tf_nlp_desaster_tweets_02-7188185c31d900af80997ad49889a931.png" width="1101" height="755"></p>
<pre><code class="language-python"># Load TensorBoard
%load_ext tensorboard
%tensorboard --logdir &#x27;./tensorboad/&#x27;
</code></pre>
<p><img alt="(Re) Introduction to Tensorflow Natural Language Processing" src="/assets/images/tf_nlp_desaster_tweets_03-ff7555ff2b5009fd2a5cdc4fbb40f8dd.png" width="847" height="649"></p>
<p><img alt="(Re) Introduction to Tensorflow Natural Language Processing" src="/assets/images/tf_nlp_desaster_tweets_04-a633f2d0c4e08c1fdf653d5c1ac6e644.png" width="844" height="581"></p>
<h2 id="saving--loading-trained-model">Saving &amp; Loading Trained Model</h2>
<h3 id="hdf5-format-higher-compatibility-to-3rd-parties">HDF5 Format (Higher Compatibility to 3rd Parties)</h3>
<pre><code class="language-python"># save best performing model to HDF5 format
model_6a.save(&#x27;./models/6a_use_added_dense.h5&#x27;)
</code></pre>
<pre><code class="language-python"># restore model with custom TF Hub layer (only HDF5 format)
# restoring requires import tensorflow_hub as hub
loaded_model_6a = tf.keras.models.load_model(
    &#x27;./models/6a_use_added_dense.h5&#x27;,
    custom_objects={&#x27;KerasLayer&#x27;: hub.KerasLayer}
)
</code></pre>
<pre><code class="language-python"># verify loaded model
loaded_model_6a.evaluate(val_tweets, val_labels) == model_6a.evaluate(val_tweets, val_labels)
# True
</code></pre>
<h3 id="saved-model-format-tensorflow-default">Saved Model Format (Tensorflow Default)</h3>
<pre><code class="language-python"># save best performing model to saved_model format
model_6a.save(&#x27;./models/6a_use_added_dense&#x27;)
</code></pre>
<pre><code class="language-python">loaded_model_6a_saved_model = tf.keras.models.load_model(
    &#x27;./models/6a_use_added_dense&#x27;,
)
</code></pre>
<pre><code class="language-python"># verify loaded model
loaded_model_6a_saved_model.evaluate(val_tweets, val_labels) == model_6a.evaluate(val_tweets, val_labels)
# True
</code></pre>
<h2 id="best-model-evaluation">Best Model Evaluation</h2>
<pre><code class="language-python"># find most wrong predictions
## create dataframe with validation tweets, labels and model predictions
loaded_model_pred_probs = tf.squeeze(loaded_model_6a_saved_model.predict(val_tweets))
loaded_model_preds = tf.round(loaded_model_preds)
print(loaded_model_preds[:5])
# [0. 1. 1. 0. 1.]
</code></pre>
<pre><code class="language-python">pred_df = pd.DataFrame({
    &quot;text&quot;: val_tweets,
    &quot;target&quot;: val_labels,
    &quot;pred&quot;: loaded_model_preds,
    &quot;pred_prob&quot;: loaded_model_pred_probs
})

pred_df
</code></pre>
<table><thead><tr><th></th><th>text</th><th>target</th><th>pred</th><th>pred_prob</th></tr></thead><tbody><tr><td>0</td><td>DFR EP016 Monthly Meltdown - On Dnbheaven 2015...</td><td>0</td><td>0.0</td><td>0.191866</td></tr><tr><td>1</td><td>FedEx no longer to transport bioterror germs i...</td><td>0</td><td>1.0</td><td>0.793415</td></tr><tr><td>2</td><td>Gunmen kill four in El Salvador bus attack: Su...</td><td>1</td><td>1.0</td><td>0.992559</td></tr><tr><td>3</td><td>@camilacabello97 Internally and externally scr...</td><td>1</td><td>0.0</td><td>0.255227</td></tr><tr><td>4</td><td>Radiation emergency #preparedness starts with ...</td><td>1</td><td>1.0</td><td>0.722030</td></tr><tr><td>...</td><td></td><td></td><td></td><td></td></tr><tr><td>757</td><td>That&#x27;s the ultimate road to destruction</td><td>0</td><td>0.0</td><td>0.141685</td></tr><tr><td>758</td><td>@SetZorah dad why dont you claim me that mean ...</td><td>0</td><td>0.0</td><td>0.111817</td></tr><tr><td>759</td><td>FedEx will no longer transport bioterror patho...</td><td>0</td><td>1.0</td><td>0.898317</td></tr><tr><td>760</td><td>Crack in the path where I wiped out this morni...</td><td>0</td><td>1.0</td><td>0.671607</td></tr><tr><td>761</td><td>I liked a @YouTube video from @dannyonpc http:...</td><td>0</td><td>0.0</td><td>0.127992</td></tr></tbody></table>
<pre><code class="language-python"># create another datframe that only contains wrong predictions
most_wrong = pred_df[pred_df[&#x27;target&#x27;] != pred_df[&#x27;pred&#x27;]].sort_values(
    &#x27;pred_prob&#x27;,
    ascending=False
)

most_wrong
</code></pre>
<table><thead><tr><th></th><th>text</th><th>target</th><th>pred</th><th>pred_prob</th></tr></thead><tbody><tr><td>31</td><td>? High Skies - Burning Buildings ? <a href="http://t.co">http://t.co</a>...</td><td>0</td><td>1.0</td><td>0.936771</td></tr><tr><td>628</td><td>@noah_anyname That&#x27;s where the concentration c...</td><td>0</td><td>1.0</td><td>0.907564</td></tr><tr><td>759</td><td>FedEx will no longer transport bioterror patho...</td><td>0</td><td>1.0</td><td>0.898317</td></tr><tr><td>251</td><td>@AshGhebranious civil rights continued in the ...</td><td>0</td><td>1.0</td><td>0.854326</td></tr><tr><td>49</td><td>@madonnamking RSPCA site multiple 7 story high...</td><td>0</td><td>1.0</td><td>0.853781</td></tr><tr><td>...</td><td></td><td></td><td></td><td></td></tr><tr><td>233</td><td>I get to smoke my shit in peace</td><td>1</td><td>0.0</td><td>0.052748</td></tr><tr><td>411</td><td>@SoonerMagic_ I mean I&#x27;m a fan but I don&#x27;t nee...</td><td>1</td><td>0.0</td><td>0.046553</td></tr><tr><td>244</td><td>Reddit Will Now Quarantine_ <a href="http://t.co/pkUA">http://t.co/pkUA</a>...</td><td>1</td><td>0.0</td><td>0.044474</td></tr><tr><td>23</td><td>Ron &amp; Fez - Dave&#x27;s High School Crush https...</td><td>1</td><td>0.0</td><td>0.040853</td></tr><tr><td>38</td><td>Why are you deluged with low self-image? Take ...</td><td>1</td><td>0.0</td><td>0.040229</td></tr></tbody></table>
<h4 id="false-positives">False Positives</h4>
<pre><code class="language-python">for row in most_wrong[:10].itertuples():
    _, text, target, pred, pred_prob = row
    print(f&quot;Target: {target}, Pred: {pred}, Probability: {pred_prob}&quot;)
    print(f&quot;Tweet: {text}&quot;)
    print(&quot;----\n&quot;)
</code></pre>
<pre><code>Target: 0, Pred: 1.0, Probability: 0.936771035194397
Tweet: ? High Skies - Burning Buildings ? http://t.co/uVq41i3Kx2 #nowplaying
----

Target: 0, Pred: 1.0, Probability: 0.9075638055801392
Tweet: @noah_anyname That&#x27;s where the concentration camps and mass murder come in. 
 
EVERY. FUCKING. TIME.
----

Target: 0, Pred: 1.0, Probability: 0.8983168601989746
Tweet: FedEx will no longer transport bioterror pathogens in wake of anthrax lab mishaps http://t.co/lHpgxc4b8J
----

Target: 0, Pred: 1.0, Probability: 0.8543258309364319
Tweet: @AshGhebranious civil rights continued in the 60s. And what about trans-generational trauma? if anything we should listen to the Americans.
----

Target: 0, Pred: 1.0, Probability: 0.8537805676460266
Tweet: @madonnamking RSPCA site multiple 7 story high rise buildings next to low density character residential in an area that floods
----

Target: 0, Pred: 1.0, Probability: 0.8336963057518005
Tweet: [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES WITH MAGNE-TRACTION INSTRUCTIONS http://t.co/xEZBs3sq0y http://t.co/C2x0QoKGlY
----

Target: 0, Pred: 1.0, Probability: 0.833143413066864
Tweet: Ashes 2015: Australias collapse at Trent Bridge among worst in history: England bundled out Australia for 60 ... http://t.co/t5TrhjUAU0
----

Target: 0, Pred: 1.0, Probability: 0.8305423855781555
Tweet: @SonofLiberty357 all illuminated by the brightly burning buildings all around the town!
----

Target: 0, Pred: 1.0, Probability: 0.8277301788330078
Tweet: Deaths 3 http://t.co/nApviyGKYK
----

Target: 0, Pred: 1.0, Probability: 0.8111591935157776
Tweet: The Sound of Arson
----
</code></pre>
<h4 id="false-negatives">False Negatives</h4>
<pre><code class="language-python">for row in most_wrong[-10:].itertuples():
    _, text, target, pred, pred_prob = row
    print(f&quot;Target: {target}, Pred: {pred}, Probability: {pred_prob}&quot;)
    print(f&quot;Tweet: {text}&quot;)
    print(&quot;----\n&quot;)
</code></pre>
<pre><code>Target: 1, Pred: 0.0, Probability: 0.07530134916305542
Tweet: going to redo my nails and watch behind the scenes of desolation of smaug ayyy
----

Target: 1, Pred: 0.0, Probability: 0.07096730172634125
Tweet: @DavidVonderhaar At least you were sincere ??
----

Target: 1, Pred: 0.0, Probability: 0.06341199576854706
Tweet: Lucas Duda is Ghost Rider. Not the Nic Cage version but an actual &#x27;engulfed in flames&#x27; badass. #Mets
----

Target: 1, Pred: 0.0, Probability: 0.06147787347435951
Tweet: You can never escape me. Bullets don&#x27;t harm me. Nothing harms me. But I know pain. I know pain. Sometimes I share it. With someone like you.
----

Target: 1, Pred: 0.0, Probability: 0.05444430932402611
Tweet: @willienelson We need help! Horses will die!Please RT &amp;amp; sign petition!Take a stand &amp;amp; be a voice for them! #gilbert23 https://t.co/e8dl1lNCVu
----

Target: 1, Pred: 0.0, Probability: 0.0527476891875267
Tweet: I get to smoke my shit in peace
----

Target: 1, Pred: 0.0, Probability: 0.04655275493860245
Tweet: @SoonerMagic_ I mean I&#x27;m a fan but I don&#x27;t need a girl sounding off like a damn siren
----

Target: 1, Pred: 0.0, Probability: 0.04447368532419205
Tweet: Reddit Will Now Quarantine_ http://t.co/pkUAMXw6pm #onlinecommunities #reddit #amageddon #freespeech #Business http://t.co/PAWvNJ4sAP
----

Target: 1, Pred: 0.0, Probability: 0.04085317254066467
Tweet: Ron &amp;amp; Fez - Dave&#x27;s High School Crush https://t.co/aN3W16c8F6 via @YouTube
----

Target: 1, Pred: 0.0, Probability: 0.0402291864156723
Tweet: Why are you deluged with low self-image? Take the quiz: http://t.co/XsPqdOrIqj http://t.co/CQYvFR4UCy
----
</code></pre>
<h2 id="test-dataset-predictions">Test Dataset Predictions</h2>
<pre><code class="language-python"># get the test dataset and randomize
test_df_shuffle = test_df.sample(frac=1, random_state=SEED)

test_df_shuffle.head(5)
</code></pre>
<table><thead><tr><th></th><th>id</th><th>keyword</th><th>location</th><th>text</th></tr></thead><tbody><tr><td>2406</td><td>8051</td><td>refugees</td><td>NaN</td><td>Refugees as citizens - The Hindu <a href="http://t.co/G">http://t.co/G</a>...</td></tr><tr><td>134</td><td>425</td><td>apocalypse</td><td>Currently Somewhere On Earth</td><td>@5SOStag honestly he could say an apocalypse i...</td></tr><tr><td>411</td><td>1330</td><td>blown%20up</td><td>Scout Team</td><td>If you bored as shit don&#x27;t nobody fuck wit you...</td></tr><tr><td>203</td><td>663</td><td>attack</td><td>NaN</td><td>@RealTwanBrown Yesterday I Had A Heat Attack ?...</td></tr><tr><td>889</td><td>2930</td><td>danger</td><td>Leeds</td><td>The Devil Wears Prada is still one of my favou...</td></tr></tbody></table>
<pre><code class="language-python">test_tweets = test_df_shuffle[&#x27;text&#x27;]
test_tweets = np.array(test_tweets.values.tolist())
</code></pre>
<pre><code class="language-python">loaded_model_pred_probs_test = tf.squeeze(loaded_model_6a_saved_model.predict(test_tweets))
loaded_model_preds_test = tf.round(loaded_model_pred_probs_test)
</code></pre>
<pre><code class="language-python">pred_test_df = pd.DataFrame({
    &quot;text&quot;: test_tweets,
    &quot;pred&quot;: loaded_model_preds_test,
    &quot;pred_prob&quot;: loaded_model_pred_probs_test
})

pred_test_df
</code></pre>
<table><thead><tr><th></th><th>text</th><th>pred</th><th>pred_prob</th></tr></thead><tbody><tr><td>0</td><td>Refugees as citizens - The Hindu <a href="http://t.co/G">http://t.co/G</a>...</td><td>1.0</td><td>0.717548</td></tr><tr><td>1</td><td>@5SOStag honestly he could say an apocalypse i...</td><td>0.0</td><td>0.093809</td></tr><tr><td>2</td><td>If you bored as shit don&#x27;t nobody fuck wit you...</td><td>0.0</td><td>0.082749</td></tr><tr><td>3</td><td>@RealTwanBrown Yesterday I Had A Heat Attack ?...</td><td>0.0</td><td>0.097784</td></tr><tr><td>4</td><td>The Devil Wears Prada is still one of my favou...</td><td>0.0</td><td>0.028455</td></tr><tr><td>...</td><td></td><td></td><td></td></tr><tr><td>3258</td><td>Free Kindle Book - Aug 3-7 - Thriller - Desola...</td><td>0.0</td><td>0.080420</td></tr><tr><td>3259</td><td>HitchBot travels Europe and greeted with open ...</td><td>1.0</td><td>0.603396</td></tr><tr><td>3260</td><td>If you told me you was drowning. I would not l...</td><td>0.0</td><td>0.208108</td></tr><tr><td>3261</td><td>First time for everything! @ Coney Island Cycl...</td><td>1.0</td><td>0.617582</td></tr><tr><td>3262</td><td>Rocky Fire #cali #SCFD #wildfire #LakeCounty h...</td><td>1.0</td><td>0.794876</td></tr></tbody></table>
<pre><code class="language-python"># make random prediction on sub-samples of the test dataset
test_samples = random.sample(test_tweets.tolist(), 10)

for test_sample in test_samples:
    pred_prob = tf.squeeze(loaded_model_6a_saved_model.predict([test_sample]))
    pred = tf.round(pred_prob)
    
    print(f&quot;Pred: {int(pred)}, Probability: {pred_prob}&quot;)
    print(f&quot;Text: {test_sample}&quot;)
    print(&quot;-------\n&quot;)
</code></pre>
<pre><code>1/1 [==============================] - 0s 26ms/step
Pred: 0, Probability: 0.15592874586582184
Text: @EllaEMusic_ You should have just simply let on that you had electrocuted yourself while plugging in your phone charger. It works for me...
-------

1/1 [==============================] - 0s 25ms/step
Pred: 1, Probability: 0.5131102800369263
Text: Evacuation drill at work. The fire doors wouldn&#x27;t open so i got to smash the emergency release glass #feelingmanly
-------

1/1 [==============================] - 0s 33ms/step
Pred: 1, Probability: 0.9751215577125549
Text: Rare photographs show the nightmare aftermath of #Hiroshima | #NoNukes #Amerikkka #WhiteTerrorism #Nuclear #Disaster  http://t.co/8tWLAKdaBf
-------

1/1 [==============================] - 0s 28ms/step
Pred: 1, Probability: 0.676400899887085
Text: kayaking about killed us so mom and grandma came to the rescue.
-------

1/1 [==============================] - 0s 26ms/step
Pred: 1, Probability: 0.9690964818000793
Text: Pasco officials impressed by drone video showing floods: Drone videos have given an up-close view of flooded areas_ http://t.co/PrUunEDids
-------

1/1 [==============================] - 0s 29ms/step
Pred: 0, Probability: 0.07793092727661133
Text: just trying to smoke and get Taco Bell
-------

1/1 [==============================] - 0s 26ms/step
Pred: 1, Probability: 0.9799614548683167
Text: RT AbbsWinston: #Zionist #Terrorist kidnapped 15 #Palestinians in overnight terror on Palestinian Villages _ http://t.co/J5mKcbKcov
-------

1/1 [==============================] - 0s 24ms/step
Pred: 1, Probability: 0.8005449771881104
Text: ME: gun shot wounds 3 4 6 7 &#x27;rapidly lethal&#x27; would have killed in 30-60 seconds or few minutes max. #kerricktrial
-------

1/1 [==============================] - 0s 24ms/step
Pred: 1, Probability: 0.9902410507202148
Text: Three Israeli soldiers wounded in West Bank terrorist attack - Haaretz http://t.co/Mwd1iPMoWT #world
-------

1/1 [==============================] - 0s 29ms/step
Pred: 0, Probability: 0.03447313234210014
Text: @_Souuul * gains super powers im now lava girl throws you ina chest wrapped in chains &amp;amp; sinks you down the the bottom of the ocean*
-------
</code></pre>
<h2 id="speedscore-tradeoff">Speed/Score Tradeoff</h2>
<pre><code class="language-python">def time_to_prediction(model, samples):
    start_time = time.perf_counter()
    model.predict(samples)
    end_time = time.perf_counter()
    time_to_prediction = end_time - start_time
    prediction_time_weighted = time_to_prediction / len(samples)

    return time_to_prediction, prediction_time_weighted
</code></pre>
<pre><code class="language-python">model_6_time_to_prediction, model_6_prediction_time_weighted = time_to_prediction(
    model = loaded_model_6a_saved_model,
    samples = test_tweets
)

print(model_6_time_to_prediction, model_6_prediction_time_weighted)
# 0.4922969689941965 0.0001508725004579211
</code></pre>
<pre><code class="language-python">model_0_time_to_prediction, model_0_prediction_time_weighted = time_to_prediction(
    model = model_0,
    samples = test_tweets
)

print(model_0_time_to_prediction, model_0_prediction_time_weighted)
# 0.05065811199892778 1.5525011338929752e-05
</code></pre>
<pre><code class="language-python">TimeToPrediction = [model_6_time_to_prediction, model_0_time_to_prediction]
TimeToPredictionWeighted = [model_6_prediction_time_weighted, model_0_prediction_time_weighted]
index = [&#x27;Model 6&#x27;, &#x27;Model 0&#x27;]

prediction_times_df = pd.DataFrame({
    &#x27;TimeToPrediction&#x27;: TimeToPrediction,
    &#x27;TimeToPrediction (weighted)&#x27;: TimeToPredictionWeighted
}, index=index)

prediction_times_df.plot(
    kind=&#x27;bar&#x27;,
    rot=0,
    subplots=True,
    figsize=(12,8)
)
</code></pre>
<p><img alt="(Re) Introduction to Tensorflow Natural Language Processing" src="/assets/images/tf_nlp_desaster_tweets_05-2ecc125f1cbd52d556aaf8c288d36525.png" width="1016" height="682"></p>
<h4 id="comparing-the-performance-of-all-models">Comparing the Performance of all Models</h4>
<pre><code class="language-python">model_1_time_to_prediction, model_1_prediction_time_weighted = time_to_prediction(
    model = model_1,
    samples = test_tweets
)

model_2_time_to_prediction, model_2_prediction_time_weighted = time_to_prediction(
    model = model_2,
    samples = test_tweets
)

model_3_time_to_prediction, model_3_prediction_time_weighted = time_to_prediction(
    model = model_3,
    samples = test_tweets
)

model_4_time_to_prediction, model_4_prediction_time_weighted = time_to_prediction(
    model = model_4,
    samples = test_tweets
)

model_5_time_to_prediction, model_5_prediction_time_weighted = time_to_prediction(
    model = model_5,
    samples = test_tweets
)

model_6_time_to_prediction, model_6_prediction_time_weighted = time_to_prediction(
    model = model_6,
    samples = test_tweets
)

model_6a_time_to_prediction, model_6a_prediction_time_weighted = time_to_prediction(
    model = model_6a,
    samples = test_tweets
)

model_6b_time_to_prediction, model_6b_prediction_time_weighted = time_to_prediction(
    model = model_6b,
    samples = test_tweets
)

model_6c_time_to_prediction, model_6c_prediction_time_weighted = time_to_prediction(
    model = model_6c,
    samples = test_tweets
)
</code></pre>
<pre><code class="language-python">plt.figure(figsize=(12, 8))
plt.scatter(model_6c_time_to_prediction, model_6c_metrics[&#x27;f1&#x27;], label=&#x27;Pre-trained USE (10%)&#x27;)
plt.scatter(model_6b_time_to_prediction, model_6b_metrics[&#x27;f1&#x27;], label=&#x27;Pre-trained USE (10% / data leakage)&#x27;)
plt.scatter(model_6a_time_to_prediction, model_6a_metrics[&#x27;f1&#x27;], label=&#x27;Pre-trained USE (added Dense)&#x27;)
plt.scatter(model_6_time_to_prediction, model_6_metrics[&#x27;f1&#x27;], label=&#x27;Pre-trained USE&#x27;)
plt.scatter(model_5_time_to_prediction, model_5_metrics[&#x27;f1&#x27;], label=&#x27;1D-Convolutional Neural Network (CNN)&#x27;)
plt.scatter(model_4_time_to_prediction, model_4_metrics[&#x27;f1&#x27;], label=&#x27;Bidirectional-LSTM model (RNN)&#x27;)
plt.scatter(model_3_time_to_prediction, model_3_metrics[&#x27;f1&#x27;], label=&#x27;GRU model (RNN)&#x27;)
plt.scatter(model_2_time_to_prediction, model_2_metrics[&#x27;f1&#x27;], label=&#x27;LSTM model (RNN)&#x27;)
plt.scatter(model_1_time_to_prediction, model_1_metrics[&#x27;f1&#x27;], label=&#x27;Feed-forward neural network (dense model)&#x27;)
plt.scatter(model_0_time_to_prediction, baseline_metrics[&#x27;f1&#x27;], label=&#x27;Naive Bayes (baseline)&#x27;)
plt.legend()
plt.title(&quot;F1-Score vs Time to Prediction&quot;)
plt.xlabel(&#x27;Time to Prediction&#x27;)
plt.ylabel(&#x27;F1-Score&#x27;)
</code></pre>
<p><img alt="(Re) Introduction to Tensorflow Natural Language Processing" src="/assets/images/tf_nlp_desaster_tweets_06-458287cf210faec316cfb51972d6c1a9.png" width="1010" height="701"></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_aHIs padding--none margin-left--sm"><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/docs/tags/python">Python</a></li><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/docs/tags/machine-learning">Machine Learning</a></li><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/docs/tags/tensorflow">Tensorflow</a></li><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/docs/tags/keras">Keras</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/IoT-and-Machine-Learning/ML/2023-04-19-tensorflow-natural-language-processing/index.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_NulP" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated__GQF"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/IoT-and-Machine-Learning/ML/2023-06-26-autogluon-transit-photometry-dataset/2023-06-26"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Detection of Exoplanets using Transit Photometry</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/IoT-and-Machine-Learning/ML/2023-04-16-deep-3d-image-segmentation/2023-04-16"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">3D Image Classification</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_IS5x thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#abstract" class="table-of-contents__link toc-highlight">Abstract</a></li><li><a href="#dataset" class="table-of-contents__link toc-highlight">Dataset</a><ul><li><a href="#exploration" class="table-of-contents__link toc-highlight">Exploration</a></li><li><a href="#train-test-split" class="table-of-contents__link toc-highlight">Train Test Split</a></li><li><a href="#tokenization-and-embedding" class="table-of-contents__link toc-highlight">Tokenization and Embedding</a></li></ul></li><li><a href="#experiments" class="table-of-contents__link toc-highlight">Experiments</a><ul><li><a href="#model-0-naive-bayes-tf-ids" class="table-of-contents__link toc-highlight">Model 0: Naive Bayes tf-ids</a></li><li><a href="#model-1-simple-dense" class="table-of-contents__link toc-highlight">Model 1: Simple Dense</a></li><li><a href="#model-2-lstm-long-term-short-term-memory-rnn" class="table-of-contents__link toc-highlight">Model 2: LSTM Long-term Short-term Memory RNN</a></li><li><a href="#model-3-gru-gated-recurrent-unit-rnn" class="table-of-contents__link toc-highlight">Model 3: GRU Gated Recurrent Unit RNN</a></li><li><a href="#model-4-bi-directional-rnn" class="table-of-contents__link toc-highlight">Model 4: Bi-Directional RNN</a></li><li><a href="#model-5-conv1d" class="table-of-contents__link toc-highlight">Model 5: Conv1D</a></li><li><a href="#model-6-transfer-learning-feature-extractor" class="table-of-contents__link toc-highlight">Model 6: Transfer Learning Feature Extractor</a></li><li><a href="#model-6a-added-dense-layer" class="table-of-contents__link toc-highlight">Model 6a (added Dense Layer)</a></li><li><a href="#model-6b-transfer-learning-feature-extractor-10-dataset" class="table-of-contents__link toc-highlight">Model 6b: Transfer Learning Feature Extractor (10% Dataset)</a></li><li><a href="#model-6c-use-data-leakage-issue-10-dataset" class="table-of-contents__link toc-highlight">Model 6c: USE Data Leakage Issue (10% Dataset)</a></li></ul></li><li><a href="#compare-experiments" class="table-of-contents__link toc-highlight">Compare Experiments</a></li><li><a href="#saving--loading-trained-model" class="table-of-contents__link toc-highlight">Saving &amp; Loading Trained Model</a><ul><li><a href="#hdf5-format-higher-compatibility-to-3rd-parties" class="table-of-contents__link toc-highlight">HDF5 Format (Higher Compatibility to 3rd Parties)</a></li><li><a href="#saved-model-format-tensorflow-default" class="table-of-contents__link toc-highlight">Saved Model Format (Tensorflow Default)</a></li></ul></li><li><a href="#best-model-evaluation" class="table-of-contents__link toc-highlight">Best Model Evaluation</a></li><li><a href="#test-dataset-predictions" class="table-of-contents__link toc-highlight">Test Dataset Predictions</a></li><li><a href="#speedscore-tradeoff" class="table-of-contents__link toc-highlight">Speed/Score Tradeoff</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Research</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Notebook</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/tags">Tags</a></li><li class="footer__item"><a class="footer__link-item" href="/Curriculum-Vitae">CV</a></li></ul></div><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.linkedin.com/in/mike-polinowski-6396ba121/" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/MikePolinowski" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.flickr.com/people/149680084@N06/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Flickr<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/mpolinowski" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_T11m"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright  2023 Mike Polinowski, INSTAR Deutschland GmbH, Shenzhen - China.</div></div></div></footer></div>
</body>
</html>